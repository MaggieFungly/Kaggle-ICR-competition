{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import ray\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from sklearn.utils import compute_class_weight, class_weight\n",
    "from sklearn.manifold import Isomap\n",
    "from typing import Tuple\n",
    "from scipy.special import expit\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['EJ'].replace(['A', 'B'], [1, 0], inplace=True)\n",
    "\n",
    "ej = np.array(train['EJ']).reshape(-1, 1)\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "y = np.array(train['Class'])\n",
    "\n",
    "greeks = pd.read_csv('greeks.csv')\n",
    "greeks['Epsilon'].replace(['Unknown'], np.nan, inplace=True)\n",
    "\n",
    "train_greeks = pd.merge(train, greeks, on='Id')\n",
    "greek_columns = greeks.columns.drop(['Id', 'Epsilon', 'Alpha']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_numerical_columns = train.drop(columns=[\"Id\", \"Class\", \"EJ\"]).columns.tolist()\n",
    "x_categorical_columns = [\"EJ\"]\n",
    "x_cols = x_numerical_columns + x_categorical_columns\n",
    "\n",
    "scaler.fit(train_greeks[x_numerical_columns])\n",
    "x_standardized = scaler.transform(train_greeks[x_numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn = KNNImputer()\n",
    "knn.fit(x_standardized)\n",
    "x_imputed_standardized = knn.transform(x_standardized)\n",
    "\n",
    "X = np.concatenate((x_imputed_standardized, ej), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(arr):\n",
    "    cols = x_numerical_columns + x_categorical_columns\n",
    "    df = pd.DataFrame(arr, columns=cols)\n",
    "    df[x_categorical_columns] = df[x_categorical_columns].astype(\"category\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancedlogloss(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    p = expit(predt)\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    grad = 1 / 2 * ((1 - y) / (1 - p) - y / p)\n",
    "    hess = 1 / 2 * ((1 - y) / ((1 - p) ** 2) + y / (p**2))\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def balancedlogloss_eval(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "    p = expit(predt)\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    return (\n",
    "        \"balanced_logloss\",\n",
    "        (-1 / n0 * (sum((1 - y) * np.log(1 - p))) - 1 / n1 * (sum(y * np.log(p)))) / 2,\n",
    "        True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3788504865670999\n"
     ]
    }
   ],
   "source": [
    "def cv(X_kf, y_kf, cv_param, num_boost_round=100):\n",
    "    \n",
    "    train_evals = pd.DataFrame()\n",
    "    test_evals = pd.DataFrame()\n",
    "\n",
    "    kf = StratifiedKFold(10, random_state=12, shuffle=True)\n",
    "    k = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(X_kf, y_kf):\n",
    "        X_train = pd.DataFrame(X_kf[train_index], columns=x_cols)\n",
    "        X_test = pd.DataFrame(X_kf[test_index], columns=x_cols)\n",
    "        y_train = y_kf[train_index]\n",
    "        y_test = y_kf[test_index]\n",
    "\n",
    "        # sampler = RandomOverSampler()\n",
    "        sampler = SMOTE()\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        n_components = 10\n",
    "        dr_cols = ['Isomap'+str(i) for i in range(n_components)]\n",
    "        dr = Isomap(n_components=n_components)\n",
    "        dr.fit(X_train)\n",
    "        X_train_dr = pd.DataFrame(dr.transform(X_train), columns=dr_cols)\n",
    "        X_test_dr = pd.DataFrame(dr.transform(X_test), columns=dr_cols)\n",
    "\n",
    "        X_train = pd.concat([X_train, X_train_dr], axis=1)\n",
    "        X_test = pd.concat([X_test, X_test_dr], axis=1)\n",
    "        cols = X_train.columns.tolist()\n",
    "\n",
    "        train_set = lgb.Dataset(X_train, y_train, feature_name=cols)\n",
    "        test_set = lgb.Dataset(X_test, y_test, feature_name=cols)\n",
    "\n",
    "        evals = {}\n",
    "        clf = lgb.train(\n",
    "                        params=cv_param,\n",
    "                        train_set=train_set,\n",
    "                        categorical_feature=['EJ'],\n",
    "                        valid_sets=[train_set, test_set],\n",
    "                        verbose_eval=False,\n",
    "                        fobj=balancedlogloss,\n",
    "                        feval=balancedlogloss_eval,\n",
    "                        num_boost_round=num_boost_round,\n",
    "                        evals_result=evals\n",
    "                        )\n",
    "\n",
    "        train_evals[str(k)] = evals[\"training\"][\"balanced_logloss\"]\n",
    "        test_evals[str(k)] = evals[\"valid_1\"][\"balanced_logloss\"]\n",
    "        k = k + 1\n",
    "\n",
    "    eval_df = pd.concat([train_evals.mean(axis=1, skipna=False), test_evals.mean(axis=1, skipna=False)], axis=1)\n",
    "    eval_df.columns = ['train', 'test']\n",
    "    return eval_df\n",
    "\n",
    "cv_params = {'learning_rate': 0.1,\n",
    "            'num_leaves': 10,\n",
    "            #  'lambda_l1': i,\n",
    "                # 'lambda_l2': 1,\n",
    "            # 'max_depth': 100,\n",
    "            'verbosity': -1\n",
    "            }\n",
    "cv_results = cv(X, y, cv_params, num_boost_round=400)\n",
    "\n",
    "print(cv_results['test'].min())\n",
    "# cv_results.iloc[range(0, len(cv_results), 50)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda_l1': 0.02, 'lambda_l2': 30, 'max_depth': 5, 'num_leaves': 10, 'verbosity': -1}\n",
      "0.4173228760123126\n",
      "{'lambda_l1': 0.02, 'lambda_l2': 30, 'max_depth': 5, 'num_leaves': 2500, 'verbosity': -1}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m params_grid \u001b[39min\u001b[39;00m grid:\n\u001b[0;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(params_grid)\n\u001b[1;32m---> 17\u001b[0m     cv_results \u001b[39m=\u001b[39m cv(X, y, params_grid, num_boost_round\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m)\n\u001b[0;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(cv_results[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmin())\n",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m, in \u001b[0;36mcv\u001b[1;34m(X_kf, y_kf, cv_param, num_boost_round)\u001b[0m\n\u001b[0;32m     20\u001b[0m dr_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mIsomap\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_components)]\n\u001b[0;32m     21\u001b[0m dr \u001b[39m=\u001b[39m Isomap(n_components\u001b[39m=\u001b[39mn_components)\n\u001b[1;32m---> 22\u001b[0m dr\u001b[39m.\u001b[39;49mfit(X_train)\n\u001b[0;32m     23\u001b[0m X_train_dr \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(dr\u001b[39m.\u001b[39mtransform(X_train), columns\u001b[39m=\u001b[39mdr_cols)\n\u001b[0;32m     24\u001b[0m X_test_dr \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(dr\u001b[39m.\u001b[39mtransform(X_test), columns\u001b[39m=\u001b[39mdr_cols)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\manifold\\_isomap.py:352\u001b[0m, in \u001b[0;36mIsomap.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39m\"\"\"Compute the embedding vectors for data X.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[39m    Returns a fitted instance of self.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 352\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X)\n\u001b[0;32m    353\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\manifold\\_isomap.py:305\u001b[0m, in \u001b[0;36mIsomap._fit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    302\u001b[0m G \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_matrix_\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    303\u001b[0m G \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[1;32m--> 305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_pca_\u001b[39m.\u001b[39;49mfit_transform(G)\n\u001b[0;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_features_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_kernel_pca.py:456\u001b[0m, in \u001b[0;36mKernelPCA.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[0;32m    436\u001b[0m     \u001b[39m\"\"\"Fit the model from data in X and transform X.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    458\u001b[0m     \u001b[39m# no need to use the kernel to transform X, use shortcut expression\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     X_transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigenvectors_ \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigenvalues_)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_kernel_pca.py:421\u001b[0m, in \u001b[0;36mKernelPCA.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_inverse_transform \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot fit_inverse_transform with a precomputed kernel.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 421\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_X)\n\u001b[0;32m    422\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_centerer \u001b[39m=\u001b[39m KernelCenterer()\n\u001b[0;32m    423\u001b[0m K \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_kernel(X)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_space = {\n",
    "    'num_leaves': [10, 2500],\n",
    "    'max_depth': [5, 80, 100],\n",
    "    'lambda_l1': [0.02, 0.5, 10],\n",
    "    'lambda_l2': [30], \n",
    "    'verbosity': [-1]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(param_space)\n",
    "\n",
    "for params_grid in grid:\n",
    "    print(params_grid)\n",
    "\n",
    "    cv_results = cv(X, y, params_grid, num_boost_round=400)\n",
    "    print(cv_results['test'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test = pd.read_csv(\"test.csv\")\n",
    "test[\"EJ\"].replace([\"A\", \"B\"], [1, 0], inplace=True)\n",
    "test_ej = np.array(test[\"EJ\"]).reshape(-1, 1)\n",
    "\n",
    "x_test_scaled = scaler.transform(test[x_numerical_columns])\n",
    "\n",
    "X_test = np.append(x_test_scaled, test_ej, axis=1) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
