{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from scipy.special import expit\n",
    "from typing import Tuple\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.utils import compute_class_weight, class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "import ray\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['EJ'].replace(['A', 'B'], [1, 0], inplace=True)\n",
    "\n",
    "ej = np.array(train['EJ']).reshape(-1, 1)\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_numerical_columns = train.drop(\n",
    "    columns=['Id', 'Class', 'EJ']).columns.tolist()\n",
    "x_categorical_columns = ['EJ']\n",
    "x_cols = x_numerical_columns + x_categorical_columns\n",
    "\n",
    "scaler.fit(train[x_numerical_columns])\n",
    "\n",
    "X = scaler.transform(train[x_numerical_columns])\n",
    "X = np.concatenate((X, ej), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn = KNNImputer()\n",
    "knn.fit(X)\n",
    "\n",
    "X = knn.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=x_cols)\n",
    "X['EJ'] = X['EJ'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = X[X > 10].dropna(how='all').dropna(how='all', axis=1)\n",
    "\n",
    "outlier_index = outlier_df.loc[(y == 0)].index.tolist()\n",
    "\n",
    "X = X.drop(index=outlier_index).reset_index(drop=True)\n",
    "y = y.drop(index=outlier_index).reset_index(drop=True)\n",
    "\n",
    "X['EJ'] = X['EJ'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancedlogloss_lgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    p = expit(predt)\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    grad = 1 / 2 * ((1 - y) / (1 - p) - y / p)\n",
    "    hess = 1 / 2 * ((1 - y) / ((1 - p) ** 2) + y / (p**2))\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def balancedlogloss_xgb(\n",
    "    predt: np.ndarray, dtrain: xgb.DMatrix\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    p = expit(predt)\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    grad = 1 / 2 * ((1 - y) / (1 - p) - y / p)\n",
    "    hess = 1 / 2 * ((1 - y) / ((1 - p) ** 2) + y / (p**2))\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def balancedlogloss_eval_lgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "    p = expit(predt)\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    return (\n",
    "        'balanced_logloss',\n",
    "        (-1 / n0 * (sum((1 - y) * np.log(1 - p))) -\n",
    "         1 / n1 * (sum(y * np.log(p)))) / 2,\n",
    "        False\n",
    "    )\n",
    "\n",
    "\n",
    "def balancedlogloss_eval_xgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "    p = expit(predt)\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    return (\n",
    "        'balanced_logloss',\n",
    "        (-1 / n0 * (sum((1 - y) * np.log(1 - p))) -\n",
    "         1 / n1 * (sum(y * np.log(p)))) / 2,\n",
    "    )\n",
    "\n",
    "\n",
    "def score(p, y):\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    return ((-1 / n0 * (sum((1 - y) * np.log(1 - p))) - 1 / n1 * (sum(y * np.log(p)))) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials_df(trials_dataframe):\n",
    "    col_index = [1] + [i for i in range(5, trials_dataframe.shape[1]-1)]\n",
    "\n",
    "    trials_dataframe = trials_dataframe.iloc[:, col_index]\n",
    "    trials_dataframe = trials_dataframe.groupby(\n",
    "        trials_dataframe.columns.tolist()[1:]).mean()\n",
    "\n",
    "    trials_dataframe = trials_dataframe.sort_values(\n",
    "        by=['value'], ascending=True)\n",
    "\n",
    "    return trials_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def xgb_objective(trial):\\n\\n    xgb_params = {\\n        \\'learning_rate\\': 0.1,\\n        \\'min_child_weight\\': trial.suggest_categorical(\\'min_child_weight\\', [i for i in range(8, 15)]),\\n        \\'reg_lambda\\': trial.suggest_float(\\'reg_lambda\\', 0.3, 1, step=0.05),\\n        \\'reg_alpha\\': trial.suggest_float(\\'reg_alpha\\', 3.5, 4.5, step=0.1),\\n        \\'max_depth\\': trial.suggest_categorical(\\'max_depth\\', [8, 10, 12]),\\n        \\'max_delta_step\\': 4,\\n        \\'subsample\\': trial.suggest_float(\\'subsample\\', 0.2, 1, step=0.1),\\n        \\'colsample_bytree\\': trial.suggest_categorical(\\'colsample_bytree\\', [0.08, 0.1, 0.12, 0.18, 0.2]),\\n        \\'disable_default_eval_metric\\': True, \\n        \\'seed\\': 5,\\n    }\\n\\n    kf = StratifiedKFold(10, shuffle=True, random_state=30)\\n    cols = X.columns.tolist()\\n\\n    xgb_scores = []\\n    \\n    for train_index, test_index in kf.split(X, y):\\n        try:\\n            X_train_val, X_test = X.loc[train_index], X.loc[test_index]\\n            y_train_val, y_test = y.loc[train_index], y.loc[test_index]\\n\\n            X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.05, \\n                                                            stratify=y_train_val, random_state=32)\\n\\n            sampler = RandomOverSampler()\\n            X_train, y_train = sampler.fit_resample(X_train, y_train)\\n\\n            n_components = 3\\n            isomap = Isomap(n_components=n_components)\\n            isomap.fit(X_train)\\n\\n            x_isomap_train = isomap.transform(X_train)\\n            x_isomap_test = isomap.transform(X_test)\\n            x_isomap_val = isomap.transform(X_val)\\n\\n            x_isomap_train = pd.DataFrame(x_isomap_train, columns=[\\'isomap_\\' + str(i) for i in range(n_components)], index=X_train.index)\\n            x_isomap_test = pd.DataFrame(x_isomap_test, columns=[\\'isomap_\\' + str(i) for i in range(n_components)], index=X_test.index)\\n            x_isomap_val = pd.DataFrame(x_isomap_val, columns=[\\'isomap_\\' + str(i) for i in range(n_components)], index=X_val.index)\\n\\n            X_train = pd.concat([X_train, x_isomap_train], axis=1)\\n            X_test = pd.concat([X_test, x_isomap_test], axis=1)\\n            X_val = pd.concat([X_val, x_isomap_val], axis=1)\\n            cols = X_train.columns.tolist()\\n\\n            dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\\n            dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\\n            dval_xgb = xgb.DMatrix(X_val, y_val, feature_names=cols, enable_categorical=True)\\n\\n            xgb_model = xgb.train(params=xgb_params,\\n                                dtrain=dtrain_xgb,\\n                                verbose_eval=False,\\n                                obj=balancedlogloss_xgb,\\n                                evals=[(dtrain_xgb, \\'train\\'), (dval_xgb, \\'validation\\')],\\n                                feval=balancedlogloss_eval_xgb,\\n                                num_boost_round=300,\\n                                early_stopping_rounds=10,\\n                                )\\n\\n            xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\\n\\n            xgb_score = score(xgb_test_preds, y_test)\\n            xgb_scores = xgb_scores + [xgb_score]\\n        \\n        except ValueError:\\n            print(\"An error occurred during Isomap fitting or transforming, skipping this part\")\\n            optuna.exceptions.TrialPruned()\\n\\n    if np.isnan(np.mean(xgb_scores)):\\n        raise optuna.exceptions.TrialPruned()\\n    \\n    return np.mean(xgb_scores)\\n\\npruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\\nxgb_study = optuna.create_study(direction=\\'minimize\\', pruner=pruner)\\nxgb_study.optimize(xgb_objective, n_trials=20)\\n\\nxgb_trials_dataframe = xgb_study.trials_dataframe()\\nget_trials_df(xgb_trials_dataframe)\\n '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def xgb_objective(trial):\n",
    "\n",
    "    xgb_params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'min_child_weight': trial.suggest_categorical('min_child_weight', [i for i in range(8, 15)]),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.3, 1, step=0.05),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 3.5, 4.5, step=0.1),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [8, 10, 12]),\n",
    "        'max_delta_step': 4,\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.08, 0.1, 0.12, 0.18, 0.2]),\n",
    "        'disable_default_eval_metric': True, \n",
    "        'seed': 5,\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(10, shuffle=True, random_state=30)\n",
    "    cols = X.columns.tolist()\n",
    "\n",
    "    xgb_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        try:\n",
    "            X_train_val, X_test = X.loc[train_index], X.loc[test_index]\n",
    "            y_train_val, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.05, \n",
    "                                                            stratify=y_train_val, random_state=32)\n",
    "\n",
    "            sampler = RandomOverSampler()\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "            n_components = 3\n",
    "            isomap = Isomap(n_components=n_components)\n",
    "            isomap.fit(X_train)\n",
    "\n",
    "            x_isomap_train = isomap.transform(X_train)\n",
    "            x_isomap_test = isomap.transform(X_test)\n",
    "            x_isomap_val = isomap.transform(X_val)\n",
    "\n",
    "            x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "            x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "            x_isomap_val = pd.DataFrame(x_isomap_val, columns=['isomap_' + str(i) for i in range(n_components)], index=X_val.index)\n",
    "\n",
    "            X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "            X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "            X_val = pd.concat([X_val, x_isomap_val], axis=1)\n",
    "            cols = X_train.columns.tolist()\n",
    "\n",
    "            dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\n",
    "            dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\n",
    "            dval_xgb = xgb.DMatrix(X_val, y_val, feature_names=cols, enable_categorical=True)\n",
    "\n",
    "            xgb_model = xgb.train(params=xgb_params,\n",
    "                                dtrain=dtrain_xgb,\n",
    "                                verbose_eval=False,\n",
    "                                obj=balancedlogloss_xgb,\n",
    "                                evals=[(dtrain_xgb, 'train'), (dval_xgb, 'validation')],\n",
    "                                feval=balancedlogloss_eval_xgb,\n",
    "                                num_boost_round=300,\n",
    "                                early_stopping_rounds=10,\n",
    "                                )\n",
    "\n",
    "            xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\n",
    "\n",
    "            xgb_score = score(xgb_test_preds, y_test)\n",
    "            xgb_scores = xgb_scores + [xgb_score]\n",
    "        \n",
    "        except ValueError:\n",
    "            print(\"An error occurred during Isomap fitting or transforming, skipping this part\")\n",
    "            optuna.exceptions.TrialPruned()\n",
    "\n",
    "    if np.isnan(np.mean(xgb_scores)):\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return np.mean(xgb_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "xgb_study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "xgb_study.optimize(xgb_objective, n_trials=20)\n",
    "\n",
    "xgb_trials_dataframe = xgb_study.trials_dataframe()\n",
    "get_trials_df(xgb_trials_dataframe)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def lgb_objective(trial):\\n\\n    lgb_params = {\\n        \\'learning_rate\\': 0.1,\\n        \\'lambda_l2\\': trial.suggest_int(\\'lambda_l2\\', 5, 20, step=3),\\n        \\'lambda_l1\\': trial.suggest_categorical(\\'lambda_l1\\', [0, 0.5]),\\n        \\'subsample\\': trial.suggest_float(\\'subsample\\', 0.1, 0.6, step=0.1),\\n        \\'colsample_bytree\\': trial.suggest_categorical(\\'colsample_bytree\\', [0.2]),\\n        \\'max_bins\\': trial.suggest_int(\\'max_bins\\', 70, 100, step=10),\\n        \\'num_leaves\\': trial.suggest_int(\\'num_leaves\\', 10, 20, step=2),\\n        \\'random_seed\\': 5,\\n        \\'first_metric_only\\': True,\\n        \\'verbosity\\': -1,\\n    }\\n\\n    n_components = trial.suggest_categorical(\\'n_components\\', [1, 2, 3])\\n    num_boost_round = trial.suggest_categorical(\\'num_boost_round\\', [90, 100, 150])\\n\\n    lgb_test_scores = []\\n    lgb_train_scores = []\\n\\n    kf = StratifiedKFold(10, shuffle=True)\\n\\n    for train_index, test_index in kf.split(X, y):\\n        \\n        X_train, X_test = X.loc[train_index], X.loc[test_index]\\n        y_train, y_test = y.loc[train_index], y.loc[test_index]\\n\\n        sampler = RandomOverSampler(random_state=3)\\n        X_train, y_train = sampler.fit_resample(X_train, y_train)\\n        \\n        try:\\n            isomap = Isomap(n_components=n_components, metric=\\'manhattan\\')\\n            isomap.fit(X_train)\\n            x_isomap_train = isomap.transform(X_train)\\n            x_isomap_test = isomap.transform(X_test)\\n\\n            x_isomap_train = pd.DataFrame(x_isomap_train, columns=[\\n                                        \\'isomap_\\' + str(i) for i in range(n_components)], index=X_train.index)\\n            x_isomap_test = pd.DataFrame(x_isomap_test, columns=[\\n                                        \\'isomap_\\' + str(i) for i in range(n_components)], index=X_test.index)\\n\\n            X_train = pd.concat([X_train, x_isomap_train], axis=1)\\n            X_test = pd.concat([X_test, x_isomap_test], axis=1)\\n\\n            dtrain_lgb = lgb.Dataset(X_train, y_train)\\n            dtest_lgb = lgb.Dataset(X_test, y_test)\\n\\n            lgb_evals = {}\\n            lgb_model = lgb.train(params=lgb_params,\\n                                train_set=dtrain_lgb,\\n                                fobj=balancedlogloss_lgb,\\n                                feval=balancedlogloss_eval_lgb,\\n                                num_boost_round=num_boost_round,\\n                                )\\n\\n            lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\\n            lgb_test_score = score(lgb_test_preds, y_test)\\n            lgb_test_scores = lgb_test_scores + [lgb_test_score]\\n\\n            lgb_train_preds = expit(lgb_model.predict(X_train, raw_score=True))\\n            lgb_train_score = score(lgb_train_preds, y_train)\\n            lgb_train_scores = lgb_train_scores + [lgb_train_score]\\n\\n        except ValueError:\\n            print(\"An error occurred during Isomap fitting or transforming, skipping this part\")\\n            optuna.exceptions.TrialPruned()\\n\\n    print((\\'train\\', np.mean(lgb_train_scores)), (\\'test\\', np.mean(lgb_test_scores)))\\n    return np.mean(lgb_test_scores)\\n\\npruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\\nlgb_study = optuna.create_study(direction=\\'minimize\\', pruner=pruner)\\nlgb_study.optimize(lgb_objective, n_trials=20)\\n\\nlgb_trials_dataframe = lgb_study.trials_dataframe()\\nget_trials_df(lgb_trials_dataframe)\\n '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def lgb_objective(trial):\n",
    "\n",
    "    lgb_params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'lambda_l2': trial.suggest_int('lambda_l2', 5, 20, step=3),\n",
    "        'lambda_l1': trial.suggest_categorical('lambda_l1', [0, 0.5]),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 0.6, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.2]),\n",
    "        'max_bins': trial.suggest_int('max_bins', 70, 100, step=10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 20, step=2),\n",
    "        'random_seed': 5,\n",
    "        'first_metric_only': True,\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "\n",
    "    n_components = trial.suggest_categorical('n_components', [1, 2, 3])\n",
    "    num_boost_round = trial.suggest_categorical('num_boost_round', [90, 100, 150])\n",
    "\n",
    "    lgb_test_scores = []\n",
    "    lgb_train_scores = []\n",
    "\n",
    "    kf = StratifiedKFold(10, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        sampler = RandomOverSampler(random_state=3)\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        try:\n",
    "            isomap = Isomap(n_components=n_components, metric='manhattan')\n",
    "            isomap.fit(X_train)\n",
    "            x_isomap_train = isomap.transform(X_train)\n",
    "            x_isomap_test = isomap.transform(X_test)\n",
    "\n",
    "            x_isomap_train = pd.DataFrame(x_isomap_train, columns=[\n",
    "                                        'isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "            x_isomap_test = pd.DataFrame(x_isomap_test, columns=[\n",
    "                                        'isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "\n",
    "            X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "            X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "\n",
    "            dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "            dtest_lgb = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "            lgb_evals = {}\n",
    "            lgb_model = lgb.train(params=lgb_params,\n",
    "                                train_set=dtrain_lgb,\n",
    "                                fobj=balancedlogloss_lgb,\n",
    "                                feval=balancedlogloss_eval_lgb,\n",
    "                                num_boost_round=num_boost_round,\n",
    "                                )\n",
    "\n",
    "            lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\n",
    "            lgb_test_score = score(lgb_test_preds, y_test)\n",
    "            lgb_test_scores = lgb_test_scores + [lgb_test_score]\n",
    "\n",
    "            lgb_train_preds = expit(lgb_model.predict(X_train, raw_score=True))\n",
    "            lgb_train_score = score(lgb_train_preds, y_train)\n",
    "            lgb_train_scores = lgb_train_scores + [lgb_train_score]\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"An error occurred during Isomap fitting or transforming, skipping this part\")\n",
    "            optuna.exceptions.TrialPruned()\n",
    "\n",
    "    print(('train', np.mean(lgb_train_scores)), ('test', np.mean(lgb_test_scores)))\n",
    "    return np.mean(lgb_test_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "lgb_study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "lgb_study.optimize(lgb_objective, n_trials=20)\n",
    "\n",
    "lgb_trials_dataframe = lgb_study.trials_dataframe()\n",
    "get_trials_df(lgb_trials_dataframe)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb: 0.28280554972081023\n",
      "lgb: 0.2911328917019051\n",
      "ensemble: 0.3113368065707118\n",
      "xgb: 0.4547847927768122\n",
      "lgb: 0.6609721867080084\n",
      "ensemble: 0.6182513794652376\n",
      "xgb: 0.31423385756598277\n",
      "lgb: 0.37113181006799434\n",
      "ensemble: 0.3809068012353133\n",
      "xgb: 0.3942536670502953\n",
      "lgb: 0.40883087578970784\n",
      "ensemble: 0.4300709206205331\n",
      "xgb: 0.24189116617156703\n",
      "lgb: 0.19297906698568193\n",
      "ensemble: 0.16462273866119823\n",
      "xgb: 0.32290211665274743\n",
      "lgb: 0.23507824562940272\n",
      "ensemble: 0.2750940167375478\n",
      "xgb: 0.29484458106219186\n",
      "lgb: 0.3329104837048046\n",
      "ensemble: 0.34190271713819503\n",
      "xgb: 0.24837211495851114\n",
      "lgb: 0.20443313305696392\n",
      "ensemble: 0.21678901555792562\n",
      "xgb: 0.2917008557356894\n",
      "lgb: 0.30645423707940506\n",
      "ensemble: 0.284575197499307\n",
      "xgb: 0.22682511887513102\n",
      "lgb: 0.15484390234350615\n",
      "ensemble: 0.1798776344623449\n",
      "\n",
      "\n",
      "xgb: 0.30726138205697384\n",
      "lgb: 0.315876683306738\n",
      "ensemble: 0.3203427227948314\n"
     ]
    }
   ],
   "source": [
    "\"\"\" xgb_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': xgb_study.best_params['min_child_weight'],\n",
    "    'reg_lambda': xgb_study.best_params['reg_lambda'],\n",
    "    'reg_alpha': xgb_study.best_params['reg_alpha'],\n",
    "    'max_depth': xgb_study.best_params['max_depth'],\n",
    "    'max_delta_step': 4,\n",
    "    'subsample': xgb_study.best_params['subsample'],\n",
    "    'colsample_bytree': xgb_study.best_params['colsample_bytree'],\n",
    "    'disable_default_eval_metric': True, \n",
    "    'seed': 5,\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'lambda_l2': lgb_study.best_params['lambda_l2'],\n",
    "    'lambda_l1': lgb_study.best_params['lambda_l1'],\n",
    "    'subsample': lgb_study.best_params['subsample'],\n",
    "    'colsample_bytree': lgb_study.best_params['colsample_bytree'],\n",
    "    'max_bins': lgb_study.best_params['max_bins'],\n",
    "    'num_leaves': lgb_study.best_params['num_leaves'],\n",
    "    'random_seed': 5,\n",
    "    'first_metric_only': True,\n",
    "    'verbosity': -1,\n",
    "} \"\"\"\n",
    "\n",
    "xgb_params = {\n",
    " 'learning_rate': 0.1,\n",
    " 'min_child_weight': 13,\n",
    " 'reg_lambda': 0.75,\n",
    " 'reg_alpha': 4.5,\n",
    " 'max_depth': 8,\n",
    " 'max_delta_step': 4,\n",
    " 'subsample': 0.4,\n",
    " 'colsample_bytree': 0.18,\n",
    " 'disable_default_eval_metric': True,\n",
    " 'seed': 5}\n",
    "\n",
    "lgb_params = {'learning_rate': 0.1,\n",
    " 'lambda_l2': 5,\n",
    " 'lambda_l1': 0.5,\n",
    " 'subsample': 0.4,\n",
    " 'colsample_bytree': 0.2,\n",
    " 'max_bins': 80,\n",
    " 'num_leaves': 12,\n",
    " 'random_seed': 5,\n",
    " 'first_metric_only': True,\n",
    " 'verbosity': -1}\n",
    "\n",
    "kf = StratifiedKFold(10, shuffle=True, random_state=30)\n",
    "cols = X.columns.tolist()\n",
    "\n",
    "df_xgb_train, df_xgb_test = pd.DataFrame(), pd.DataFrame()\n",
    "df_lgb_train, df_lgb_test = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "xgb_test_scores = []\n",
    "lgb_test_scores = []\n",
    "ensemble_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    sampler = RandomOverSampler()\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    n_components = 3\n",
    "    isomap = Isomap(n_components=n_components)\n",
    "    isomap.fit(X_train)\n",
    "\n",
    "    x_isomap_train = isomap.transform(X_train)\n",
    "    x_isomap_test = isomap.transform(X_test)\n",
    "\n",
    "    x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "    x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "\n",
    "    X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "    X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "    cols = X_train.columns.tolist()\n",
    "\n",
    "    evals_xgb = {}\n",
    "    dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\n",
    "    dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\n",
    "\n",
    "    xgb_model = xgb.train(params=xgb_params,\n",
    "                          dtrain=dtrain_xgb,\n",
    "                          obj=balancedlogloss_xgb,\n",
    "                          verbose_eval=False,\n",
    "                          feval=balancedlogloss_eval_xgb,\n",
    "                          evals_result=evals_xgb,\n",
    "                          num_boost_round=100,\n",
    "                          )\n",
    "    \n",
    "    xgb_train_preds = expit(xgb_model.predict(dtrain_xgb, output_margin=True))\n",
    "    xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\n",
    "\n",
    "    xgb_test_score = score(xgb_test_preds, y_test)\n",
    "    xgb_test_scores = xgb_test_scores + [xgb_test_score]\n",
    "    print(f'xgb: {xgb_test_score}')\n",
    "\n",
    "    evals_lgb = {}\n",
    "    dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "    dtest_lgb = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                          train_set=dtrain_lgb,\n",
    "                          fobj=balancedlogloss_lgb,\n",
    "                          feval=balancedlogloss_eval_lgb,\n",
    "                          evals_result=evals_lgb,\n",
    "                          num_boost_round=100,\n",
    "                          verbose_eval=False)\n",
    "\n",
    "    lgb_train_preds = expit(lgb_model.predict(X_train, raw_score=True))\n",
    "    lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\n",
    "\n",
    "    lgb_test_score = score(lgb_test_preds, y_test)\n",
    "    lgb_test_scores = lgb_test_scores + [lgb_test_score]\n",
    "    print(f'lgb: {lgb_test_score}')\n",
    "\n",
    "    stacked_preds_train = np.column_stack(((expit(xgb_train_preds)), (expit(lgb_train_preds))))\n",
    "    stacked_preds_test = np.column_stack(((expit(xgb_test_preds)), (expit(lgb_test_preds))))\n",
    "\n",
    "    meta_model = LogisticRegression(C=5)\n",
    "    # meta_model = xgb.XGBClassifier()\n",
    "    meta_model.fit(stacked_preds_train, y_train)\n",
    "    ensemble_preds = meta_model.predict_proba(stacked_preds_test)[:, 1]\n",
    "\n",
    "    ensemble_score = score(ensemble_preds, np.array(y_test))\n",
    "    ensemble_scores = ensemble_scores + [ensemble_score]\n",
    "    print(f'ensemble: {ensemble_score}')\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(f'xgb: {np.mean(xgb_test_scores)}')\n",
    "print(f'lgb: {np.mean(lgb_test_scores)}')\n",
    "print(f'ensemble: {np.mean(ensemble_scores)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
