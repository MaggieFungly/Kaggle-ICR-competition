{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from scipy.special import expit\n",
    "from typing import Tuple\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import compute_class_weight, class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "import ray\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['EJ'].replace(['A', 'B'], [1, 0], inplace=True)\n",
    "\n",
    "ej = np.array(train['EJ']).reshape(-1, 1)\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "x_numerical_columns = train.drop(\n",
    "    columns=['Id', 'Class', 'EJ']).columns.tolist()\n",
    "x_categorical_columns = ['EJ']\n",
    "x_cols = x_numerical_columns + x_categorical_columns\n",
    "\n",
    "scaler.fit(train[x_numerical_columns])\n",
    "\n",
    "X = scaler.transform(train[x_numerical_columns])\n",
    "X = np.concatenate((X, ej), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn = KNNImputer(n_neighbors=5)\n",
    "knn.fit(X)\n",
    "\n",
    "X = knn.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=x_cols)\n",
    "X['EJ'] = X['EJ'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = X[X > 10].dropna(how='all').dropna(how='all', axis=1)\n",
    "\n",
    "outlier_index = outlier_df.loc[(y == 0)].index.tolist()\n",
    "\n",
    "X = X.drop(index=outlier_index).reset_index(drop=True)\n",
    "y = y.drop(index=outlier_index).reset_index(drop=True)\n",
    "\n",
    "X['EJ'] = X['EJ'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancedlogloss_lgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    p = expit(predt)\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    grad = 1 / 2 * ((1 - y) / (1 - p) - y / p)\n",
    "    hess = 1 / 2 * ((1 - y) / ((1 - p) ** 2) + y / (p**2))\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def balancedlogloss_xgb(\n",
    "    predt: np.ndarray, dtrain: xgb.DMatrix\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    p = expit(predt)\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    grad = 1 / 2 * ((1 - y) / (1 - p) - y / p)\n",
    "    hess = 1 / 2 * ((1 - y) / ((1 - p) ** 2) + y / (p**2))\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def balancedlogloss_eval_lgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "    p = expit(predt)\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    return (\n",
    "        'balanced_logloss',\n",
    "        (-1 / n0 * (sum((1 - y) * np.log(1 - p))) -\n",
    "         1 / n1 * (sum(y * np.log(p)))) / 2,\n",
    "        False\n",
    "    )\n",
    "\n",
    "\n",
    "def balancedlogloss_eval_xgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "    p = expit(predt)\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    return (\n",
    "        'balanced_logloss',\n",
    "        (-1 / n0 * (sum((1 - y) * np.log(1 - p))) -\n",
    "         1 / n1 * (sum(y * np.log(p)))) / 2,\n",
    "    )\n",
    "\n",
    "\n",
    "def score(p, y):\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "    p[p == 1] = 1-(1e-15)\n",
    "\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    return ((-1 / n0 * (sum((1 - y) * np.log(1 - p))) - 1 / n1 * (sum(y * np.log(p)))) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials_df(trials_dataframe):\n",
    "    col_index = [1] + [i for i in range(5, trials_dataframe.shape[1]-1)]\n",
    "\n",
    "    trials_dataframe = trials_dataframe.iloc[:, col_index]\n",
    "    trials_dataframe = trials_dataframe.groupby(\n",
    "        trials_dataframe.columns.tolist()[1:]).mean()\n",
    "\n",
    "    trials_dataframe = trials_dataframe.sort_values(\n",
    "        by=['value'], ascending=True)\n",
    "\n",
    "    return trials_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 17:49:32,691] A new study created in memory with name: no-name-f4e92176-8015-41db-8c57-54ff6cda4ec8\n",
      "[I 2023-07-01 17:49:46,143] Trial 0 finished with value: 0.23531573945472223 and parameters: {'min_child_weight': 14, 'reg_lambda': 2.0, 'reg_alpha': 0.4, 'max_depth': 3, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.2, 'num_boost_round': 200, 'n_components': 10}. Best is trial 0 with value: 0.23531573945472223.\n",
      "[I 2023-07-01 17:50:00,334] Trial 1 finished with value: 0.2537906785149122 and parameters: {'min_child_weight': 8, 'reg_lambda': 1.8000000000000003, 'reg_alpha': 1.4000000000000001, 'max_depth': 3, 'subsample': 0.6000000000000001, 'colsample_bytree': 0.2, 'num_boost_round': 200, 'n_components': 10}. Best is trial 0 with value: 0.23531573945472223.\n",
      "[I 2023-07-01 17:50:17,428] Trial 2 finished with value: 0.243061913548709 and parameters: {'min_child_weight': 11, 'reg_lambda': 1.1, 'reg_alpha': 0.6, 'max_depth': 3, 'subsample': 0.2, 'colsample_bytree': 0.3, 'num_boost_round': 150, 'n_components': 1}. Best is trial 0 with value: 0.23531573945472223.\n",
      "[I 2023-07-01 17:50:32,970] Trial 3 finished with value: 0.23460425087775602 and parameters: {'min_child_weight': 10, 'reg_lambda': 1.6, 'reg_alpha': 0.4, 'max_depth': 20, 'subsample': 0.7, 'colsample_bytree': 0.4, 'num_boost_round': 100, 'n_components': 3}. Best is trial 3 with value: 0.23460425087775602.\n",
      "[I 2023-07-01 17:50:47,467] Trial 4 finished with value: 0.22987130040469098 and parameters: {'min_child_weight': 14, 'reg_lambda': 1.5000000000000002, 'reg_alpha': 1.6, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 0.4, 'num_boost_round': 150, 'n_components': 1}. Best is trial 4 with value: 0.22987130040469098.\n",
      "[I 2023-07-01 17:51:03,380] Trial 5 finished with value: 0.24922129050466674 and parameters: {'min_child_weight': 9, 'reg_lambda': 0.8, 'reg_alpha': 1.5000000000000002, 'max_depth': 20, 'subsample': 0.2, 'colsample_bytree': 0.2, 'num_boost_round': 100, 'n_components': 1}. Best is trial 4 with value: 0.22987130040469098.\n",
      "[I 2023-07-01 17:51:18,449] Trial 6 finished with value: 0.26531860467421275 and parameters: {'min_child_weight': 12, 'reg_lambda': 1.7000000000000002, 'reg_alpha': 1.2000000000000002, 'max_depth': 3, 'subsample': 0.9000000000000001, 'colsample_bytree': 0.15, 'num_boost_round': 150, 'n_components': 10}. Best is trial 4 with value: 0.22987130040469098.\n",
      "[I 2023-07-01 17:51:32,725] Trial 7 finished with value: 0.2569758916521158 and parameters: {'min_child_weight': 10, 'reg_lambda': 0.7000000000000001, 'reg_alpha': 0.7000000000000001, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.15, 'num_boost_round': 150, 'n_components': 3}. Best is trial 4 with value: 0.22987130040469098.\n",
      "[I 2023-07-01 17:51:34,834] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred skipping this part\n",
      "Input X contains infinity or a value too large for dtype('float64').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 17:51:48,274] Trial 9 finished with value: 0.25195508658592397 and parameters: {'min_child_weight': 8, 'reg_lambda': 1.9000000000000001, 'reg_alpha': 1.7000000000000002, 'max_depth': 3, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.4, 'num_boost_round': 100, 'n_components': 3}. Best is trial 4 with value: 0.22987130040469098.\n",
      "[I 2023-07-01 17:52:02,605] Trial 10 finished with value: 0.23724493653744433 and parameters: {'min_child_weight': 14, 'reg_lambda': 0.30000000000000004, 'reg_alpha': 1.0, 'max_depth': 8, 'subsample': 1.0, 'colsample_bytree': 0.4, 'num_boost_round': 150, 'n_components': 1}. Best is trial 4 with value: 0.22987130040469098.\n",
      "[I 2023-07-01 17:52:17,731] Trial 11 finished with value: 0.27366601202190427 and parameters: {'min_child_weight': 10, 'reg_lambda': 1.3000000000000003, 'reg_alpha': 0.2, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.4, 'num_boost_round': 100, 'n_components': 3}. Best is trial 4 with value: 0.22987130040469098.\n",
      "[I 2023-07-01 17:52:32,510] Trial 12 finished with value: 0.2389377460895651 and parameters: {'min_child_weight': 13, 'reg_lambda': 1.4000000000000001, 'reg_alpha': 2.0, 'max_depth': 20, 'subsample': 0.5, 'colsample_bytree': 0.4, 'num_boost_round': 100, 'n_components': 3}. Best is trial 4 with value: 0.22987130040469098.\n"
     ]
    }
   ],
   "source": [
    "def xgb_objective(trial):\n",
    "\n",
    "    xgb_params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'min_child_weight': trial.suggest_categorical('min_child_weight', [i for i in range(8, 15)]),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 2, step=0.1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 2, step=0.1),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 8, 10, 20]),\n",
    "        'max_delta_step': 4,\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.15, 0.2, 0.3, 0.4]),\n",
    "        'disable_default_eval_metric': True, \n",
    "        'seed': 5,\n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_categorical('num_boost_round', [100, 150, 200])\n",
    "\n",
    "    n_components = trial.suggest_categorical('n_components', [1, 3, 10])\n",
    "\n",
    "    kf = StratifiedKFold(10, shuffle=True, random_state=30)\n",
    "    cols = X.columns.tolist()\n",
    "\n",
    "    xgb_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        try:\n",
    "            X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "            y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "            sampler = RandomOverSampler()\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "            isomap = Isomap(n_components=n_components)\n",
    "            isomap.fit(np.array(X_train).astype('float64'))\n",
    "\n",
    "            x_isomap_train = isomap.transform(X_train)\n",
    "            x_isomap_test = isomap.transform(X_test)\n",
    "\n",
    "            x_isomap_train = pd.DataFrame(x_isomap_train, \n",
    "                                          columns=['isomap_' + str(i) for i in range(n_components)], \n",
    "                                          index=X_train.index)\n",
    "            x_isomap_test = pd.DataFrame(x_isomap_test, \n",
    "                                         columns=['isomap_' + str(i) for i in range(n_components)], \n",
    "                                         index=X_test.index)\n",
    "\n",
    "            X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "            X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "\n",
    "            cols = X_train.columns.tolist()\n",
    "\n",
    "            dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\n",
    "            dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\n",
    "\n",
    "            xgb_model = xgb.train(params=xgb_params,\n",
    "                                  dtrain=dtrain_xgb,\n",
    "                                  verbose_eval=False,\n",
    "                                  obj=balancedlogloss_xgb,\n",
    "                                  feval=balancedlogloss_eval_xgb,\n",
    "                                  num_boost_round=100,\n",
    "                                )\n",
    "\n",
    "            xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\n",
    "            xgb_score = score(xgb_test_preds, y_test)\n",
    "            xgb_scores = xgb_scores + [xgb_score]\n",
    "        \n",
    "        except ValueError as e:\n",
    "            print(\"An error occurred skipping this part\")\n",
    "            print(e)\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    if np.isnan(np.mean(xgb_scores)):\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return np.mean(xgb_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "xgb_study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "xgb_study.optimize(xgb_objective, n_trials=100)\n",
    "\n",
    "xgb_trials_dataframe = xgb_study.trials_dataframe()\n",
    "get_trials_df(xgb_trials_dataframe).to_excel('xgb_trials.xlsx')\n",
    "get_trials_df(xgb_trials_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def lgb_objective(trial):\n",
    "\n",
    "    lgb_params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'lambda_l2': trial.suggest_int('lambda_l2', 5, 20, step=3),\n",
    "        'lambda_l1': trial.suggest_categorical('lambda_l1', [0, 0.5]),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 0.6, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.2]),\n",
    "        'max_bins': trial.suggest_int('max_bins', 70, 100, step=10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 20, step=2),\n",
    "        'random_seed': 5,\n",
    "        'first_metric_only': True,\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "\n",
    "    n_components = trial.suggest_categorical('n_components', [1, 2, 3])\n",
    "    num_boost_round = trial.suggest_categorical('num_boost_round', [90, 100, 150])\n",
    "\n",
    "    lgb_test_scores = []\n",
    "    lgb_train_scores = []\n",
    "\n",
    "    kf = StratifiedKFold(10, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        sampler = RandomOverSampler(random_state=3)\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        try:\n",
    "            isomap = Isomap(n_components=n_components, metric='manhattan')\n",
    "            isomap.fit(X_train)\n",
    "            x_isomap_train = isomap.transform(X_train)\n",
    "            x_isomap_test = isomap.transform(X_test)\n",
    "\n",
    "            x_isomap_train = pd.DataFrame(x_isomap_train, columns=[\n",
    "                                        'isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "            x_isomap_test = pd.DataFrame(x_isomap_test, columns=[\n",
    "                                        'isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "\n",
    "            X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "            X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "\n",
    "            dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "            dtest_lgb = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "            lgb_evals = {}\n",
    "            lgb_model = lgb.train(params=lgb_params,\n",
    "                                train_set=dtrain_lgb,\n",
    "                                fobj=balancedlogloss_lgb,\n",
    "                                feval=balancedlogloss_eval_lgb,\n",
    "                                num_boost_round=num_boost_round,\n",
    "                                )\n",
    "\n",
    "            lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\n",
    "            lgb_test_score = score(lgb_test_preds, y_test)\n",
    "            lgb_test_scores = lgb_test_scores + [lgb_test_score]\n",
    "\n",
    "            lgb_train_preds = expit(lgb_model.predict(X_train, raw_score=True))\n",
    "            lgb_train_score = score(lgb_train_preds, y_train)\n",
    "            lgb_train_scores = lgb_train_scores + [lgb_train_score]\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"An error occurred during Isomap fitting or transforming, skipping this part\")\n",
    "            optuna.exceptions.TrialPruned()\n",
    "\n",
    "    print(('train', np.mean(lgb_train_scores)), ('test', np.mean(lgb_test_scores)))\n",
    "    return np.mean(lgb_test_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "lgb_study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "lgb_study.optimize(lgb_objective, n_trials=20)\n",
    "\n",
    "lgb_trials_dataframe = lgb_study.trials_dataframe()\n",
    "get_trials_df(lgb_trials_dataframe)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': xgb_study.best_params['min_child_weight'],\n",
    "    'reg_lambda': xgb_study.best_params['reg_lambda'],\n",
    "    'reg_alpha': xgb_study.best_params['reg_alpha'],\n",
    "    'max_depth': xgb_study.best_params['max_depth'],\n",
    "    'max_delta_step': 4,\n",
    "    'subsample': xgb_study.best_params['subsample'],\n",
    "    'colsample_bytree': xgb_study.best_params['colsample_bytree'],\n",
    "    'disable_default_eval_metric': True, \n",
    "    'seed': 5,\n",
    "}\n",
    "\n",
    "\"\"\" lgb_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'lambda_l2': lgb_study.best_params['lambda_l2'],\n",
    "    'lambda_l1': lgb_study.best_params['lambda_l1'],\n",
    "    'subsample': lgb_study.best_params['subsample'],\n",
    "    'colsample_bytree': lgb_study.best_params['colsample_bytree'],\n",
    "    'max_bins': lgb_study.best_params['max_bins'],\n",
    "    'num_leaves': lgb_study.best_params['num_leaves'],\n",
    "    'random_seed': 5,\n",
    "    'first_metric_only': True,\n",
    "    'verbosity': -1,\n",
    "} \"\"\"\n",
    "\n",
    "\"\"\" xgb_params = {\n",
    " 'learning_rate': 0.1,\n",
    " 'min_child_weight': 13,\n",
    " 'reg_lambda': 0.75,\n",
    " 'reg_alpha': 4.5,\n",
    " 'max_depth': 8,\n",
    " 'max_delta_step': 4,\n",
    " 'subsample': 0.4,\n",
    " 'colsample_bytree': 0.18,\n",
    " 'disable_default_eval_metric': True,\n",
    " 'seed': 5} \"\"\"\n",
    "\n",
    "lgb_params = {'learning_rate': 0.1,\n",
    " 'lambda_l2': 5,\n",
    " 'lambda_l1': 0.5,\n",
    " 'subsample': 0.4,\n",
    " 'colsample_bytree': 0.2,\n",
    " 'max_bins': 80,\n",
    " 'num_leaves': 12,\n",
    " 'random_seed': 5,\n",
    " 'first_metric_only': True,\n",
    " 'verbosity': -1}\n",
    "\n",
    "kf = StratifiedKFold(10, shuffle=True, random_state=30)\n",
    "cols = X.columns.tolist()\n",
    "\n",
    "\n",
    "xgb_test_scores = []\n",
    "lgb_test_scores = []\n",
    "logistic_test_scores = []\n",
    "ensemble_scores = []\n",
    "mlp_test_scores = []\n",
    "\n",
    "k = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "    print(f'Fold {k}')\n",
    "    k = k + 1\n",
    "\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    sampler = RandomOverSampler()\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    n_components = 3\n",
    "    isomap = Isomap(n_components=n_components)\n",
    "    isomap.fit(X_train)\n",
    "\n",
    "    x_isomap_train = isomap.transform(X_train)\n",
    "    x_isomap_test = isomap.transform(X_test)\n",
    "\n",
    "    x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "    x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "\n",
    "    X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "    X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "    cols = X_train.columns.tolist()\n",
    "\n",
    "    evals_xgb = {}\n",
    "    dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\n",
    "    dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\n",
    "\n",
    "    xgb_model = xgb.train(params=xgb_params,\n",
    "                          dtrain=dtrain_xgb,\n",
    "                          obj=balancedlogloss_xgb,\n",
    "                          verbose_eval=10,\n",
    "                          feval=balancedlogloss_eval_xgb,\n",
    "                          evals_result=evals_xgb,\n",
    "                          num_boost_round=100,\n",
    "                          )\n",
    "    \n",
    "    xgb_train_preds = expit(xgb_model.predict(dtrain_xgb, output_margin=True))\n",
    "    xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\n",
    "\n",
    "    xgb_test_score = score(xgb_test_preds, y_test)\n",
    "    xgb_test_scores = xgb_test_scores + [xgb_test_score]\n",
    "    print(f'xgb: {xgb_test_score}')\n",
    "   \n",
    "    evals_lgb = {}\n",
    "    dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "    dtest_lgb = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                          train_set=dtrain_lgb,\n",
    "                          fobj=balancedlogloss_lgb,\n",
    "                          feval=balancedlogloss_eval_lgb,\n",
    "                          evals_result=evals_lgb,\n",
    "                          num_boost_round=100,\n",
    "                          verbose_eval=False)\n",
    "\n",
    "    lgb_train_preds = expit(lgb_model.predict(X_train, raw_score=True))\n",
    "    lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\n",
    "\n",
    "    lgb_test_score = score(lgb_test_preds, y_test)\n",
    "    lgb_test_scores = lgb_test_scores + [lgb_test_score]\n",
    "    print(f'lgb: {lgb_test_score}')\n",
    "\n",
    "    \"\"\" mlp = MLPClassifier(hidden_layer_sizes=(100, 100), \n",
    "                        max_iter=1000,\n",
    "                        # alpha=0.5,\n",
    "                        early_stopping=True,\n",
    "                        )\n",
    "\n",
    "    mlp.fit(x_isomap_train, y_train)\n",
    "    mlp_train_preds = mlp.predict_proba(x_isomap_train)[:, 1]\n",
    "    mlp_test_preds = mlp.predict_proba(x_isomap_test)[:, 1]\n",
    "\n",
    "    mlp_train_score = score(mlp_train_preds, y_train)\n",
    "    mlp_test_score = score(mlp_test_preds, y_test)\n",
    "    mlp_test_scores = mlp_test_scores + [mlp_test_score]\n",
    "    print(f'mlp_train: {mlp_train_score} \\t mlp_test: {mlp_test_score}') \"\"\"\n",
    "\n",
    "    preds_train_mean = np.mean([xgb_train_preds, lgb_train_preds], axis=0)\n",
    "    preds_test_mean = np.mean([xgb_test_preds, lgb_test_preds], axis=0)\n",
    "    stacked_preds_train = np.column_stack((xgb_train_preds, lgb_train_preds, preds_train_mean))\n",
    "    stacked_preds_test = np.column_stack((xgb_test_preds, lgb_test_preds, preds_test_mean))\n",
    "\n",
    "    meta_model = LogisticRegression(C=0.2)\n",
    "    meta_model.fit(stacked_preds_train, y_train)\n",
    "    ensemble_preds = meta_model.predict_proba(stacked_preds_test)[:, 1]\n",
    "\n",
    "    ensemble_score = score(ensemble_preds, np.array(y_test))\n",
    "    ensemble_scores = ensemble_scores + [ensemble_score]\n",
    "    print(f'ensemble: {ensemble_score}')\n",
    "\n",
    "print(f'xgb: {np.mean(xgb_test_scores)}')\n",
    "print(f'lgb: {np.mean(lgb_test_scores)}')\n",
    "# print(f'logistic: {np.mean(logistic_test_scores)}')\n",
    "# print(f'mlp: {np.mean(mlp_test_scores)}')\n",
    "print(f'ensemble: {np.mean(ensemble_scores)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
