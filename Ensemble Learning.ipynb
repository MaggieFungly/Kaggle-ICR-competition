{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import optuna\n",
    "import ray\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from sklearn.utils import compute_class_weight, class_weight\n",
    "from sklearn.manifold import Isomap\n",
    "from typing import Tuple\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.manifold import Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['EJ'].replace(['A', 'B'], [1, 0], inplace=True)\n",
    "\n",
    "ej = np.array(train['EJ']).reshape(-1, 1)\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_numerical_columns = train.drop(columns=['Id', 'Class', 'EJ']).columns.tolist()\n",
    "x_categorical_columns = ['EJ']\n",
    "x_cols = x_numerical_columns + x_categorical_columns\n",
    "\n",
    "scaler.fit(train[x_numerical_columns])\n",
    "\n",
    "X = scaler.transform(train[x_numerical_columns])\n",
    "X = np.concatenate((X, ej), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn = KNNImputer()\n",
    "knn.fit(X)\n",
    "\n",
    "X = knn.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=x_cols)\n",
    "X['EJ'] = X['EJ'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = X[X>10].dropna(how='all').dropna(how='all', axis=1)\n",
    "\n",
    "outlier_index = outlier_df.loc[(y==0)].index.tolist()\n",
    "\n",
    "X = X.drop(index=outlier_index).reset_index(drop=True)\n",
    "y = y.drop(index=outlier_index).reset_index(drop=True)\n",
    "\n",
    "X['EJ'] = X['EJ'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancedlogloss_lgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    p = expit(predt)\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    grad = 1 / 2 * ((1 - y) / (1 - p) - y / p)\n",
    "    hess = 1 / 2 * ((1 - y) / ((1 - p) ** 2) + y / (p**2))\n",
    "    return grad, hess\n",
    "\n",
    "def balancedlogloss_xgb(\n",
    "    predt: np.ndarray, dtrain: xgb.DMatrix\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    p = expit(predt)\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    grad = 1 / 2 * ((1 - y) / (1 - p) - y / p)\n",
    "    hess = 1 / 2 * ((1 - y) / ((1 - p) ** 2) + y / (p**2))\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def balancedlogloss_eval_lgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "    p = expit(predt)\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    return (\n",
    "        'balanced_logloss',\n",
    "        (-1/ n0 * (sum((1 - y) * np.log(1 - p))) - 1 / n1 * (sum(y * np.log(p)))) / 2,\n",
    "        False\n",
    "    )\n",
    "\n",
    "def balancedlogloss_eval_xgb(\n",
    "    predt: np.ndarray, dtrain: lgb.Dataset\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = dtrain.get_label()\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "    p = expit(predt)\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    return (\n",
    "        'balanced_logloss',\n",
    "        (-1 / n0 * (sum((1 - y) * np.log(1 - p))) - 1 / n1 * (sum(y * np.log(p)))) / 2,\n",
    "    )\n",
    "\n",
    "def score(p, y):\n",
    "\n",
    "    p[p == 0] = 1e-15\n",
    "\n",
    "    n0 = len(y[y == 0])\n",
    "    n1 = len(y[y == 1])\n",
    "\n",
    "    return ((-1/ n0 * (sum((1 - y) * np.log(1 - p))) - 1 / n1 * (sum(y * np.log(p)))) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials_df(trials_dataframe):\n",
    "    col_index = [1] + [i for i in range(5, trials_dataframe.shape[1]-1)]\n",
    "\n",
    "    trials_dataframe = trials_dataframe.iloc[:, col_index]\n",
    "    trials_dataframe = trials_dataframe.groupby(trials_dataframe.columns.tolist()[1:]).mean()\n",
    "\n",
    "    trials_dataframe = trials_dataframe.sort_values(by=['value'], ascending=True)\n",
    "\n",
    "    return trials_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def xgb_objective(trial):\\n\\n    xgb_params = {\\n        'learning_rate': 0.1,\\n        'min_child_weight': trial.suggest_categorical('min_child_weight', [i for i in range(8, 15)]),\\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.3, 1, step=0.05),\\n        'reg_alpha': trial.suggest_float('reg_alpha', 3.5, 4.5, step=0.1),\\n        'max_depth': trial.suggest_categorical('max_depth', [8, 10, 12]),\\n        'max_delta_step': 4,\\n        'subsample': trial.suggest_float('subsample', 0.2, 1, step=0.1),\\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.08, 0.1, 0.12, 0.18, 0.2]),\\n        'disable_default_eval_metric': True, \\n        'seed': 5,\\n    }\\n\\n    kf = StratifiedKFold(10, shuffle=True, random_state=30)\\n    cols = X.columns.tolist()\\n\\n    xgb_scores = []\\n    \\n    for train_index, test_index in kf.split(X, y):\\n        X_train_val, X_test = X.loc[train_index], X.loc[test_index]\\n        y_train_val, y_test = y.loc[train_index], y.loc[test_index]\\n\\n        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.05, \\n                                                          stratify=y_train_val, random_state=32)\\n\\n        sampler = RandomOverSampler()\\n        X_train, y_train = sampler.fit_resample(X_train, y_train)\\n\\n        n_components = 3\\n        isomap = Isomap(n_components=n_components)\\n        isomap.fit(X_train)\\n\\n        x_isomap_train = isomap.transform(X_train)\\n        x_isomap_test = isomap.transform(X_test)\\n        x_isomap_val = isomap.transform(X_val)\\n\\n        x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\\n        x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\\n        x_isomap_val = pd.DataFrame(x_isomap_val, columns=['isomap_' + str(i) for i in range(n_components)], index=X_val.index)\\n\\n        X_train = pd.concat([X_train, x_isomap_train], axis=1)\\n        X_test = pd.concat([X_test, x_isomap_test], axis=1)\\n        X_val = pd.concat([X_val, x_isomap_val], axis=1)\\n        cols = X_train.columns.tolist()\\n\\n        dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\\n        dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\\n        dval_xgb = xgb.DMatrix(X_val, y_val, feature_names=cols, enable_categorical=True)\\n\\n        xgb_model = xgb.train(params=xgb_params,\\n                            dtrain=dtrain_xgb,\\n                            verbose_eval=False,\\n                            obj=balancedlogloss_xgb,\\n                            evals=[(dtrain_xgb, 'train'), (dval_xgb, 'validation')],\\n                            feval=balancedlogloss_eval_xgb,\\n                            num_boost_round=300,\\n                            early_stopping_rounds=10,\\n                            )\\n\\n        xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\\n\\n        xgb_score = score(xgb_test_preds, y_test)\\n        xgb_scores = xgb_scores + [xgb_score]\\n\\n    if np.isnan(np.mean(xgb_scores)):\\n        raise optuna.exceptions.TrialPruned()\\n    \\n    return np.mean(xgb_scores)\\n\\npruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\\nxgb_study = optuna.create_study(direction='minimize', pruner=pruner)\\nxgb_study.optimize(xgb_objective, n_trials=50)\\n\\nxgb_trials_dataframe = xgb_study.trials_dataframe()\\nget_trials_df(xgb_trials_dataframe)\\n \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def xgb_objective(trial):\n",
    "\n",
    "    xgb_params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'min_child_weight': trial.suggest_categorical('min_child_weight', [i for i in range(8, 15)]),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.3, 1, step=0.05),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 3.5, 4.5, step=0.1),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [8, 10, 12]),\n",
    "        'max_delta_step': 4,\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.08, 0.1, 0.12, 0.18, 0.2]),\n",
    "        'disable_default_eval_metric': True, \n",
    "        'seed': 5,\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(10, shuffle=True, random_state=30)\n",
    "    cols = X.columns.tolist()\n",
    "\n",
    "    xgb_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train_val, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train_val, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.05, \n",
    "                                                          stratify=y_train_val, random_state=32)\n",
    "\n",
    "        sampler = RandomOverSampler()\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        n_components = 3\n",
    "        isomap = Isomap(n_components=n_components)\n",
    "        isomap.fit(X_train)\n",
    "\n",
    "        x_isomap_train = isomap.transform(X_train)\n",
    "        x_isomap_test = isomap.transform(X_test)\n",
    "        x_isomap_val = isomap.transform(X_val)\n",
    "\n",
    "        x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "        x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "        x_isomap_val = pd.DataFrame(x_isomap_val, columns=['isomap_' + str(i) for i in range(n_components)], index=X_val.index)\n",
    "\n",
    "        X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "        X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "        X_val = pd.concat([X_val, x_isomap_val], axis=1)\n",
    "        cols = X_train.columns.tolist()\n",
    "\n",
    "        dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\n",
    "        dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\n",
    "        dval_xgb = xgb.DMatrix(X_val, y_val, feature_names=cols, enable_categorical=True)\n",
    "\n",
    "        xgb_model = xgb.train(params=xgb_params,\n",
    "                            dtrain=dtrain_xgb,\n",
    "                            verbose_eval=False,\n",
    "                            obj=balancedlogloss_xgb,\n",
    "                            evals=[(dtrain_xgb, 'train'), (dval_xgb, 'validation')],\n",
    "                            feval=balancedlogloss_eval_xgb,\n",
    "                            num_boost_round=300,\n",
    "                            early_stopping_rounds=10,\n",
    "                            )\n",
    "\n",
    "        xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\n",
    "\n",
    "        xgb_score = score(xgb_test_preds, y_test)\n",
    "        xgb_scores = xgb_scores + [xgb_score]\n",
    "\n",
    "    if np.isnan(np.mean(xgb_scores)):\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return np.mean(xgb_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "xgb_study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "xgb_study.optimize(xgb_objective, n_trials=50)\n",
    "\n",
    "xgb_trials_dataframe = xgb_study.trials_dataframe()\n",
    "get_trials_df(xgb_trials_dataframe)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-28 17:40:27,869] A new study created in memory with name: no-name-e01a8a75-6381-4dbd-8432-dba72093598a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-28 17:40:41,888] Trial 0 finished with value: 0.49466552714659606 and parameters: {'min_child_weight': 20, 'lambda_l2': 20, 'lambda_l1': 20, 'max_depth': 2, 'subsample': 0.1, 'colsample_bytree': 0.1}. Best is trial 0 with value: 0.49466552714659606.\n",
      "[I 2023-06-28 17:40:58,532] Trial 1 finished with value: 0.41596755737994046 and parameters: {'min_child_weight': 1, 'lambda_l2': 30, 'lambda_l1': 20, 'max_depth': 2, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.30000000000000004}. Best is trial 1 with value: 0.41596755737994046.\n",
      "[I 2023-06-28 17:41:17,174] Trial 2 finished with value: 0.36224269200502685 and parameters: {'min_child_weight': 1, 'lambda_l2': 20, 'lambda_l1': 10, 'max_depth': 10, 'subsample': 0.2, 'colsample_bytree': 0.30000000000000004}. Best is trial 2 with value: 0.36224269200502685.\n",
      "[I 2023-06-28 17:41:34,358] Trial 3 finished with value: 0.3303467169231117 and parameters: {'min_child_weight': 5, 'lambda_l2': 30, 'lambda_l1': 10, 'max_depth': 5, 'subsample': 0.9, 'colsample_bytree': 0.30000000000000004}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:41:51,314] Trial 4 finished with value: 0.35787511212550766 and parameters: {'min_child_weight': 10, 'lambda_l2': 30, 'lambda_l1': 10, 'max_depth': 5, 'subsample': 0.4, 'colsample_bytree': 0.2}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:42:08,063] Trial 5 finished with value: 0.37886769180741503 and parameters: {'min_child_weight': 20, 'lambda_l2': 0, 'lambda_l1': 40, 'max_depth': 5, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.2}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:42:25,761] Trial 6 finished with value: 0.3996254885557467 and parameters: {'min_child_weight': 20, 'lambda_l2': 0, 'lambda_l1': 40, 'max_depth': 10, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.2}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:42:48,171] Trial 7 finished with value: 0.331482925421562 and parameters: {'min_child_weight': 1, 'lambda_l2': 10, 'lambda_l1': 10, 'max_depth': 20, 'subsample': 1.0, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:43:12,044] Trial 8 finished with value: 0.3670918409414988 and parameters: {'min_child_weight': 5, 'lambda_l2': 20, 'lambda_l1': 30, 'max_depth': 5, 'subsample': 0.1, 'colsample_bytree': 0.5}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:43:29,834] Trial 9 finished with value: 0.5002486659780048 and parameters: {'min_child_weight': 1, 'lambda_l2': 10, 'lambda_l1': 0, 'max_depth': 20, 'subsample': 0.6, 'colsample_bytree': 0.1}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:43:51,671] Trial 10 finished with value: 0.36204379562691347 and parameters: {'min_child_weight': 5, 'lambda_l2': 40, 'lambda_l1': 0, 'max_depth': 5, 'subsample': 1.0, 'colsample_bytree': 0.5}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:44:15,619] Trial 11 finished with value: 0.34017712500755504 and parameters: {'min_child_weight': 5, 'lambda_l2': 10, 'lambda_l1': 10, 'max_depth': 20, 'subsample': 1.0, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:44:44,099] Trial 12 finished with value: 0.3621349301337495 and parameters: {'min_child_weight': 10, 'lambda_l2': 40, 'lambda_l1': 10, 'max_depth': 20, 'subsample': 0.9, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:45:07,644] Trial 13 finished with value: 0.34393410865567636 and parameters: {'min_child_weight': 1, 'lambda_l2': 30, 'lambda_l1': 0, 'max_depth': 20, 'subsample': 0.8, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:45:29,381] Trial 14 finished with value: 0.389968949326408 and parameters: {'min_child_weight': 5, 'lambda_l2': 10, 'lambda_l1': 20, 'max_depth': 5, 'subsample': 0.9, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:45:52,980] Trial 15 finished with value: 0.3888482866770008 and parameters: {'min_child_weight': 1, 'lambda_l2': 30, 'lambda_l1': 30, 'max_depth': 20, 'subsample': 0.8, 'colsample_bytree': 0.30000000000000004}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:46:14,906] Trial 16 finished with value: 0.3509544966307033 and parameters: {'min_child_weight': 5, 'lambda_l2': 10, 'lambda_l1': 10, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:46:40,246] Trial 17 finished with value: 0.422581267492468 and parameters: {'min_child_weight': 10, 'lambda_l2': 40, 'lambda_l1': 30, 'max_depth': 2, 'subsample': 1.0, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:47:10,802] Trial 18 finished with value: 0.3603829330981648 and parameters: {'min_child_weight': 5, 'lambda_l2': 20, 'lambda_l1': 0, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.30000000000000004}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:47:33,243] Trial 19 finished with value: 0.38020853608629296 and parameters: {'min_child_weight': 1, 'lambda_l2': 0, 'lambda_l1': 10, 'max_depth': 20, 'subsample': 0.9, 'colsample_bytree': 0.2}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:48:00,431] Trial 20 finished with value: 0.34396508027957323 and parameters: {'min_child_weight': 5, 'lambda_l2': 30, 'lambda_l1': 20, 'max_depth': 20, 'subsample': 0.5, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:48:19,510] Trial 21 finished with value: 0.3435292397214379 and parameters: {'min_child_weight': 5, 'lambda_l2': 10, 'lambda_l1': 10, 'max_depth': 20, 'subsample': 1.0, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:48:38,042] Trial 22 finished with value: 0.3520425288388144 and parameters: {'min_child_weight': 5, 'lambda_l2': 10, 'lambda_l1': 10, 'max_depth': 20, 'subsample': 1.0, 'colsample_bytree': 0.30000000000000004}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:49:00,484] Trial 23 finished with value: 0.3701924042322217 and parameters: {'min_child_weight': 5, 'lambda_l2': 20, 'lambda_l1': 0, 'max_depth': 20, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:49:17,639] Trial 24 finished with value: 0.3600080051213821 and parameters: {'min_child_weight': 5, 'lambda_l2': 10, 'lambda_l1': 10, 'max_depth': 20, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:49:36,743] Trial 25 finished with value: 0.3617666926841952 and parameters: {'min_child_weight': 1, 'lambda_l2': 0, 'lambda_l1': 20, 'max_depth': 5, 'subsample': 1.0, 'colsample_bytree': 0.30000000000000004}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:49:52,232] Trial 26 finished with value: 0.4011405749552014 and parameters: {'min_child_weight': 20, 'lambda_l2': 10, 'lambda_l1': 10, 'max_depth': 2, 'subsample': 0.8, 'colsample_bytree': 0.4}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:50:10,856] Trial 27 finished with value: 0.3621425244281875 and parameters: {'min_child_weight': 10, 'lambda_l2': 20, 'lambda_l1': 20, 'max_depth': 10, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 3 with value: 0.3303467169231117.\n",
      "[I 2023-06-28 17:50:28,871] Trial 28 finished with value: 0.31976645721198854 and parameters: {'min_child_weight': 5, 'lambda_l2': 0, 'lambda_l1': 0, 'max_depth': 20, 'subsample': 1.0, 'colsample_bytree': 0.4}. Best is trial 28 with value: 0.31976645721198854.\n"
     ]
    }
   ],
   "source": [
    "def lgb_objective(trial):\n",
    "\n",
    "    lgb_params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'min_child_weight': trial.suggest_categorical('min_child_weight', [1, 5, 10, 20]),\n",
    "        'lambda_l2': trial.suggest_int('lambda_l2', 0, 40, step=10),\n",
    "        'lambda_l1': trial.suggest_int('lambda_l1', 0, 40, step=10),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [2, 5, 10, 20]),\n",
    "        'max_delta_step': 4,\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5, step=0.1),\n",
    "        'random_seed': 5,\n",
    "        'first_metric_only': True,\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(10, shuffle=True, random_state=30)\n",
    "    cols = X.columns.tolist()\n",
    "\n",
    "    lgb_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train_val, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train_val, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.05, \n",
    "                                                          stratify=y_train_val, random_state=32)\n",
    "\n",
    "        sampler = RandomOverSampler()\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        try:\n",
    "            n_components = 3\n",
    "            isomap = Isomap(n_components=n_components)\n",
    "            isomap.fit(X_train)\n",
    "\n",
    "            x_isomap_train = isomap.transform(X_train)\n",
    "            x_isomap_test = isomap.transform(X_test)\n",
    "            x_isomap_val = isomap.transform(X_val)\n",
    "\n",
    "            x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "            x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "            x_isomap_val = pd.DataFrame(x_isomap_val, columns=['isomap_' + str(i) for i in range(n_components)], index=X_val.index)\n",
    "\n",
    "            X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "            X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "            X_val = pd.concat([X_val, x_isomap_val], axis=1)\n",
    "        \n",
    "        except ValueError as e:\n",
    "            print(\"An error occurred during Isomap fitting or transforming, skipping this fold. Error: \", str(e))\n",
    "\n",
    "\n",
    "        cols = X_train.columns.tolist()\n",
    "\n",
    "        dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "        dtest_lgb = lgb.Dataset(X_test, y_test)\n",
    "        dval_lgb = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "        lgb_evals = {}\n",
    "        lgb_model = lgb.train(params=lgb_params,\n",
    "                            train_set=dtrain_lgb,\n",
    "                            valid_sets=[dtrain_lgb, dval_lgb],\n",
    "                            fobj=balancedlogloss_lgb,\n",
    "                            feval=balancedlogloss_eval_lgb,\n",
    "                            num_boost_round=100,\n",
    "                            early_stopping_rounds=2,\n",
    "                            verbose_eval=False,\n",
    "                            evals_result=lgb_evals,\n",
    "                            )\n",
    "\n",
    "        lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\n",
    "\n",
    "        lgb_score = score(lgb_test_preds, y_test)\n",
    "        lgb_scores = lgb_scores + [lgb_score]\n",
    "        \n",
    "    if np.isnan(np.mean(lgb_scores)):\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return np.mean(lgb_scores)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "lgb_study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "lgb_study.optimize(lgb_objective, n_trials=50)\n",
    "\n",
    "lgb_trials_dataframe = lgb_study.trials_dataframe()\n",
    "get_trials_df(lgb_trials_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" lgb_params = {'learning_rate': 0.1,\\n              'lambda_l1': 30,\\n              'lambda_l2': 10,\\n              'verbosity': -1,\\n              'first_metric_only': True\\n              }\\n\\n\\nlgb_scores = []\\nlgb_train_df = pd.DataFrame()\\nlgb_eval_df = pd.DataFrame()\\n\\nfor train_index, test_index in kf.split(X, y):\\n    X_train_val, X_test = X.loc[train_index], X.loc[test_index]\\n    y_train_val, y_test = y.loc[train_index], y.loc[test_index]\\n\\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.03, \\n                                                        stratify=y_train_val, random_state=32)\\n\\n    sampler = RandomOverSampler()\\n    X_train, y_train = sampler.fit_resample(X_train, y_train)\\n\\n    n_components = 10\\n    isomap = Isomap(n_components=n_components)\\n    isomap.fit(X_train)\\n\\n    x_isomap_train = isomap.transform(X_train)\\n    x_isomap_test = isomap.transform(X_test)\\n    x_isomap_val = isomap.transform(X_val)\\n\\n    x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\\n    x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\\n    x_isomap_val = pd.DataFrame(x_isomap_val, columns=['isomap_' + str(i) for i in range(n_components)], index=X_val.index)\\n\\n    X_train = pd.concat([X_train, x_isomap_train], axis=1)\\n    X_test = pd.concat([X_test, x_isomap_test], axis=1)\\n    X_val = pd.concat([X_val, x_isomap_val], axis=1)\\n    cols = X_train.columns.tolist()\\n\\n    dtrain_lgb = lgb.Dataset(X_train, y_train)\\n    dtest_lgb = lgb.Dataset(X_test, y_test)\\n    dval_lgb = lgb.Dataset(X_val, y_val)\\n\\n    lgb_evals = {}\\n    lgb_model = lgb.train(params=lgb_params,\\n                        train_set=dtrain_lgb,\\n                        valid_sets=[dtrain_lgb, dval_lgb],\\n                        valid_names=['train', 'eval'],\\n                        fobj=balancedlogloss_lgb,\\n                        feval=balancedlogloss_eval_lgb,\\n                        num_boost_round=200,\\n                        early_stopping_rounds=5,\\n                        verbose_eval=False,\\n                        evals_result=lgb_evals,\\n                        )\\n\\n    lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\\n\\n    lgb_score = score(lgb_test_preds, y_test)\\n    lgb_scores = lgb_scores + [lgb_score]\\n\\n    lgb_train_df = pd.concat([lgb_train_df, pd.DataFrame(lgb_evals['train'])], axis=1)\\n    lgb_eval_df = pd.concat([lgb_eval_df, pd.DataFrame(lgb_evals['eval'])], axis=1)\\n\\nlgb_train_df = lgb_train_df.mean(axis=1)\\nlgb_eval_df = lgb_eval_df.mean(axis=1, skipna=False)\\n\\nprint(np.mean(lgb_scores))\\n# pd.concat([lgb_train_df, lgb_eval_df], axis=1) \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' lgb_params = {'learning_rate': 0.1,\n",
    "              'lambda_l1': 30,\n",
    "              'lambda_l2': 10,\n",
    "              'verbosity': -1,\n",
    "              'first_metric_only': True\n",
    "              }\n",
    "\n",
    "\n",
    "lgb_scores = []\n",
    "lgb_train_df = pd.DataFrame()\n",
    "lgb_eval_df = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train_val, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train_val, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.03, \n",
    "                                                        stratify=y_train_val, random_state=32)\n",
    "\n",
    "    sampler = RandomOverSampler()\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    n_components = 10\n",
    "    isomap = Isomap(n_components=n_components)\n",
    "    isomap.fit(X_train)\n",
    "\n",
    "    x_isomap_train = isomap.transform(X_train)\n",
    "    x_isomap_test = isomap.transform(X_test)\n",
    "    x_isomap_val = isomap.transform(X_val)\n",
    "\n",
    "    x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "    x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "    x_isomap_val = pd.DataFrame(x_isomap_val, columns=['isomap_' + str(i) for i in range(n_components)], index=X_val.index)\n",
    "\n",
    "    X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "    X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "    X_val = pd.concat([X_val, x_isomap_val], axis=1)\n",
    "    cols = X_train.columns.tolist()\n",
    "\n",
    "    dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "    dtest_lgb = lgb.Dataset(X_test, y_test)\n",
    "    dval_lgb = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "    lgb_evals = {}\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                        train_set=dtrain_lgb,\n",
    "                        valid_sets=[dtrain_lgb, dval_lgb],\n",
    "                        valid_names=['train', 'eval'],\n",
    "                        fobj=balancedlogloss_lgb,\n",
    "                        feval=balancedlogloss_eval_lgb,\n",
    "                        num_boost_round=200,\n",
    "                        early_stopping_rounds=5,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=lgb_evals,\n",
    "                        )\n",
    "\n",
    "    lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\n",
    "\n",
    "    lgb_score = score(lgb_test_preds, y_test)\n",
    "    lgb_scores = lgb_scores + [lgb_score]\n",
    "\n",
    "    lgb_train_df = pd.concat([lgb_train_df, pd.DataFrame(lgb_evals['train'])], axis=1)\n",
    "    lgb_eval_df = pd.concat([lgb_eval_df, pd.DataFrame(lgb_evals['eval'])], axis=1)\n",
    "\n",
    "lgb_train_df = lgb_train_df.mean(axis=1)\n",
    "lgb_eval_df = lgb_eval_df.mean(axis=1, skipna=False)\n",
    "\n",
    "print(np.mean(lgb_scores))\n",
    "# pd.concat([lgb_train_df, lgb_eval_df], axis=1) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xgb_param \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m      2\u001b[0m xgb_param[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m      3\u001b[0m xgb_param[\u001b[39m'\u001b[39m\u001b[39mmax_delta_step\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_param = study.best_params\n",
    "xgb_param['learning_rate'] = 0.1\n",
    "xgb_param['max_delta_step'] = 4\n",
    "xgb_param['seed'] = 5\n",
    "xgb_param['disable_default_eval_metric'] = True\n",
    "\n",
    "lgb_params = {'learning_rate': 0.1,\n",
    "              'lambda_l1': 0.1,\n",
    "              'verbosity': -1,\n",
    "              'first_metric_only': True\n",
    "              }\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(10, shuffle=True, random_state=30)\n",
    "cols = X.columns.tolist()\n",
    "\n",
    "df_xgb_train, df_xgb_test = pd.DataFrame(), pd.DataFrame()\n",
    "df_lgb_train, df_lgb_test = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "xgb_scores = []\n",
    "lgb_scores = []\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "    X_train_val, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train_val, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, stratify=y_train_val, test_size=0.05, random_state=32)\n",
    "\n",
    "    sampler = RandomOverSampler()\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    n_components = 3\n",
    "    isomap = Isomap(n_components=n_components)\n",
    "    isomap.fit(X_train)\n",
    "\n",
    "    x_isomap_train = isomap.transform(X_train)\n",
    "    x_isomap_test = isomap.transform(X_test)\n",
    "    x_isomap_val = isomap.transform(X_val)\n",
    "\n",
    "    x_isomap_train = pd.DataFrame(x_isomap_train, columns=['isomap_' + str(i) for i in range(n_components)], index=X_train.index)\n",
    "    x_isomap_test = pd.DataFrame(x_isomap_test, columns=['isomap_' + str(i) for i in range(n_components)], index=X_test.index)\n",
    "    x_isomap_val = pd.DataFrame(x_isomap_val, columns=['isomap_' + str(i) for i in range(n_components)], index=X_val.index)\n",
    "\n",
    "    X_train = pd.concat([X_train, x_isomap_train], axis=1)\n",
    "    X_test = pd.concat([X_test, x_isomap_test], axis=1)\n",
    "    X_val = pd.concat([X_val, x_isomap_val], axis=1)\n",
    "    cols = X_train.columns.tolist()\n",
    "\n",
    "    evals_xgb = {}\n",
    "    dtrain_xgb = xgb.DMatrix(X_train, y_train, feature_names=cols, enable_categorical=True)\n",
    "    dtest_xgb = xgb.DMatrix(X_test, y_test, feature_names=cols, enable_categorical=True)\n",
    "    dval_xgb = xgb.DMatrix(X_val, y_val, feature_names=cols, enable_categorical=True)\n",
    "\n",
    "    xgb_model = xgb.train(params=xgb_param,\n",
    "                          dtrain=dtrain_xgb,\n",
    "                          obj=balancedlogloss_xgb,\n",
    "                          verbose_eval=False,\n",
    "                          evals=[(dtrain_xgb, 'train'), (dval_xgb, 'val')],\n",
    "                          feval=balancedlogloss_eval_xgb,\n",
    "                          evals_result=evals_xgb,\n",
    "                          early_stopping_rounds=10,\n",
    "                          num_boost_round=300,\n",
    "                          )\n",
    "    \n",
    "    df_xgb_train = pd.concat([df_xgb_train, pd.Series(evals_xgb['train']['balanced_logloss'])], axis=1)\n",
    "    df_xgb_test = pd.concat([df_xgb_test, pd.Series(evals_xgb['val']['balanced_logloss'])], axis=1)\n",
    "\n",
    "    xgb_train_preds = expit(xgb_model.predict(dtrain_xgb, output_margin=True))\n",
    "    xgb_test_preds = expit(xgb_model.predict(dtest_xgb, output_margin=True))\n",
    "\n",
    "    xgb_score = score(xgb_test_preds, y_test)\n",
    "    xgb_scores = xgb_scores + [xgb_score]\n",
    "    print(xgb_score)\n",
    "\n",
    "    evals_lgb = {}\n",
    "    dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "    dtest_lgb = lgb.Dataset(X_test, y_test)\n",
    "    dval_lgb = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "    lgb_model = lgb.train(params=lgb_param,\n",
    "                          train_set=dtrain_lgb,\n",
    "                          valid_sets=[dtrain_lgb, dval_lgb],\n",
    "                          fobj=balancedlogloss_lgb,\n",
    "                          feval=balancedlogloss_eval_lgb,\n",
    "                          evals_result=evals_lgb,\n",
    "                          valid_names=['train', 'val'],\n",
    "                          num_boost_round=500,\n",
    "                          verbose_eval=False)\n",
    "\n",
    "    df_lgb_train = pd.concat([df_lgb_train, pd.Series(evals_lgb['train']['balanced_logloss'])], axis=1)\n",
    "    df_lgb_test = pd.concat([df_lgb_test, pd.Series(evals_lgb['val']['balanced_logloss'])], axis=1)\n",
    "\n",
    "    lgb_train_preds = expit(lgb_model.predict(X_train, raw_score=True))\n",
    "    lgb_test_preds = expit(lgb_model.predict(X_test, raw_score=True))\n",
    "\n",
    "    lgb_score = score(lgb_test_preds, y_test)\n",
    "    lgb_scores = lgb_scores + [lgb_score]\n",
    "    print(lgb_score)\n",
    "\n",
    "    stacked_preds_train = np.column_stack(((expit(xgb_train_preds)), (expit(lgb_train_preds))))\n",
    "    stacked_preds_test = np.column_stack(((expit(xgb_test_preds)), (expit(lgb_test_preds))))\n",
    "\n",
    "    meta_model = LogisticRegression(C=10, random_state=20)\n",
    "    # meta_model = xgb.XGBClassifier()\n",
    "    meta_model.fit(stacked_preds_train, y_train)\n",
    "    ensemble_preds = meta_model.predict_proba(stacked_preds_test)[:, 1]\n",
    "\n",
    "    ensemble_score = score(ensemble_preds, np.array(y_test))\n",
    "    scores = scores + [ensemble_score]\n",
    "    print('ensemble: ' + str(ensemble_score))\n",
    "\n",
    "df_xgb = pd.DataFrame()\n",
    "df_xgb['train'] = df_xgb_train.mean(axis=1)\n",
    "df_xgb['val'] = df_xgb_test.mean(axis=1)\n",
    "\n",
    "df_lgb = pd.DataFrame()\n",
    "df_lgb['train'] = df_lgb_train.mean(axis=1)\n",
    "df_lgb['val'] = df_lgb_test.mean(axis=1)\n",
    "\n",
    "print('\\n')\n",
    "print('xgb: ' + str(np.mean(xgb_scores)))\n",
    "print('lgb: ' + str(np.mean(lgb_scores)))\n",
    "print('ensemble:' + str(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
