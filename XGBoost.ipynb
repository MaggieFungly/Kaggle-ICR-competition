{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:27:34.402365Z","iopub.status.busy":"2023-06-06T05:27:34.401962Z","iopub.status.idle":"2023-06-06T05:27:34.410334Z","shell.execute_reply":"2023-06-06T05:27:34.408963Z","shell.execute_reply.started":"2023-06-06T05:27:34.402334Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import sklearn\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import lightgbm as lgb\n","import optuna\n","from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n","from imblearn.over_sampling import RandomOverSampler\n","\n","from typing import Tuple\n","from scipy.special import expit\n","import xgboost as xgb"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:27:34.413098Z","iopub.status.busy":"2023-06-06T05:27:34.412022Z","iopub.status.idle":"2023-06-06T05:27:34.447005Z","shell.execute_reply":"2023-06-06T05:27:34.445774Z","shell.execute_reply.started":"2023-06-06T05:27:34.413052Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('train.csv')\n","\n","train['EJ'].replace(['A', 'B'], [1, 0], inplace=True)\n","\n","ej = np.array(pd.get_dummies(train['EJ']))\n","\n","sample_submission = pd.read_csv('sample_submission.csv')\n","\n","y = np.array(train['Class'])"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:27:34.448654Z","iopub.status.busy":"2023-06-06T05:27:34.448325Z","iopub.status.idle":"2023-06-06T05:27:34.464173Z","shell.execute_reply":"2023-06-06T05:27:34.462969Z","shell.execute_reply.started":"2023-06-06T05:27:34.448626Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","\n","x_numerical_columns = train.drop(columns=['Id', 'Class', 'EJ']).columns\n","\n","scaler.fit(train[x_numerical_columns])\n","x_standardized = scaler.transform(train[x_numerical_columns])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:27:34.466139Z","iopub.status.busy":"2023-06-06T05:27:34.465670Z","iopub.status.idle":"2023-06-06T05:27:34.498551Z","shell.execute_reply":"2023-06-06T05:27:34.496919Z","shell.execute_reply.started":"2023-06-06T05:27:34.466101Z"},"trusted":true},"outputs":[],"source":["from sklearn.impute import KNNImputer\n","\n","knn = KNNImputer()\n","knn.fit(x_standardized)\n","x_imputed_standardized = knn.transform(x_standardized)\n","\n","X = np.append(x_imputed_standardized, ej, axis=1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:27:34.506288Z","iopub.status.busy":"2023-06-06T05:27:34.505109Z","iopub.status.idle":"2023-06-06T05:27:34.528982Z","shell.execute_reply":"2023-06-06T05:27:34.526816Z","shell.execute_reply.started":"2023-06-06T05:27:34.506227Z"},"trusted":true},"outputs":[],"source":["def balancedlogloss(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n","    y = dtrain.get_label()\n","    n0 = len(y[y==0])\n","    n1 = len(y[y==1])\n","\n","    p = expit(predt)\n","\n","    p[p==0] = 1e-15\n","\n","    grad = 1/2*((1-y)/(1-p)-y/p)\n","    hess = 1/2*((1-y)/((1-p)**2)+y/(p**2))\n","    return grad, hess\n","\n","def scoring(y, p):\n","\n","    p = expit(p)\n","\n","    p[p==0] = 1e-15\n","\n","    n0 = len(y[y==0])\n","    n1 = len(y[y==1])\n","    \n","    return (-1/n0*(sum((1-y)*np.log(1-p)))-1/n1*(sum(y*np.log(p))))/2\n","\n","def balancedlogloss_eval(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n","    y = dtrain.get_label()\n","    n0 = len(y[y==0])\n","    n1 = len(y[y==1])\n","    p = expit(predt)\n","\n","    p[p==0] = 1e-15\n","\n","    return 'balanced_logloss', (-1/n0*(sum((1-y)*np.log(1-p)))-1/n1*(sum(y*np.log(p))))/2"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:27:34.531944Z","iopub.status.busy":"2023-06-06T05:27:34.531254Z","iopub.status.idle":"2023-06-06T05:28:24.904528Z","shell.execute_reply":"2023-06-06T05:28:24.903306Z","shell.execute_reply.started":"2023-06-06T05:27:34.531880Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2023-06-26 12:53:25,439] A new study created in memory with name: no-name-e5987ef2-84cb-40ba-90f8-c32a1c95aca2\n","[I 2023-06-26 12:53:26,946] Trial 0 finished with value: 0.48704556233704227 and parameters: {'learning_rate': 0.089, 'gamma': 0.401, 'reg_lambda': 41.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 4}. Best is trial 0 with value: 0.48704556233704227.\n","[I 2023-06-26 12:53:29,129] Trial 1 finished with value: 0.6206910432256842 and parameters: {'learning_rate': 0.341, 'gamma': 0.20600000000000002, 'reg_lambda': 91.0, 'max_depth': 8, 'min_child_weight': 0.8, 'max_delta_step': 1}. Best is trial 0 with value: 0.48704556233704227.\n","[I 2023-06-26 12:53:31,324] Trial 2 finished with value: 0.5272619943253083 and parameters: {'learning_rate': 0.053000000000000005, 'gamma': 1.8459999999999999, 'reg_lambda': 61.0, 'max_depth': 7, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 3}. Best is trial 0 with value: 0.48704556233704227.\n","[I 2023-06-26 12:53:32,547] Trial 3 finished with value: 0.6485003800048179 and parameters: {'learning_rate': 0.017, 'gamma': 0.151, 'reg_lambda': 31.0, 'max_depth': 4, 'min_child_weight': 0.9, 'max_delta_step': 5}. Best is trial 0 with value: 0.48704556233704227.\n","[I 2023-06-26 12:53:33,655] Trial 4 finished with value: 0.39261462229183686 and parameters: {'learning_rate': 0.14100000000000001, 'gamma': 1.506, 'reg_lambda': 91.0, 'max_depth': 3, 'min_child_weight': 0.1, 'max_delta_step': 2}. Best is trial 4 with value: 0.39261462229183686.\n","[I 2023-06-26 12:53:35,787] Trial 5 finished with value: 0.7475818273146597 and parameters: {'learning_rate': 0.421, 'gamma': 0.106, 'reg_lambda': 61.0, 'max_depth': 7, 'min_child_weight': 0.8, 'max_delta_step': 1}. Best is trial 4 with value: 0.39261462229183686.\n","[I 2023-06-26 12:53:37,851] Trial 6 finished with value: 0.7622725794157518 and parameters: {'learning_rate': 0.445, 'gamma': 0.921, 'reg_lambda': 71.0, 'max_depth': 7, 'min_child_weight': 0.6, 'max_delta_step': 5}. Best is trial 4 with value: 0.39261462229183686.\n","[I 2023-06-26 12:53:39,700] Trial 7 finished with value: 0.6428811804509321 and parameters: {'learning_rate': 0.257, 'gamma': 1.281, 'reg_lambda': 21.0, 'max_depth': 8, 'min_child_weight': 0.4, 'max_delta_step': 3}. Best is trial 4 with value: 0.39261462229183686.\n","[I 2023-06-26 12:53:41,134] Trial 8 finished with value: 0.4061636401700208 and parameters: {'learning_rate': 0.10500000000000001, 'gamma': 0.741, 'reg_lambda': 91.0, 'max_depth': 4, 'min_child_weight': 0.8, 'max_delta_step': 4}. Best is trial 4 with value: 0.39261462229183686.\n","[I 2023-06-26 12:53:43,219] Trial 9 finished with value: 1.0021674140514347 and parameters: {'learning_rate': 0.429, 'gamma': 1.756, 'reg_lambda': 1.0, 'max_depth': 4, 'min_child_weight': 0.8, 'max_delta_step': 1}. Best is trial 4 with value: 0.39261462229183686.\n","[I 2023-06-26 12:53:44,333] Trial 10 finished with value: 0.3738839981251299 and parameters: {'learning_rate': 0.181, 'gamma': 1.401, 'reg_lambda': 71.0, 'max_depth': 2, 'min_child_weight': 0.1, 'max_delta_step': 2}. Best is trial 10 with value: 0.3738839981251299.\n","[I 2023-06-26 12:53:45,441] Trial 11 finished with value: 0.3731125079300088 and parameters: {'learning_rate': 0.213, 'gamma': 1.396, 'reg_lambda': 81.0, 'max_depth': 2, 'min_child_weight': 0.1, 'max_delta_step': 2}. Best is trial 11 with value: 0.3731125079300088.\n","[I 2023-06-26 12:53:50,042] Trial 12 finished with value: 0.41945751038992507 and parameters: {'learning_rate': 0.20500000000000002, 'gamma': 1.291, 'reg_lambda': 71.0, 'max_depth': 10, 'min_child_weight': 0.1, 'max_delta_step': 2}. Best is trial 11 with value: 0.3731125079300088.\n","[I 2023-06-26 12:53:51,052] Trial 13 finished with value: 0.3725294493490526 and parameters: {'learning_rate': 0.241, 'gamma': 1.276, 'reg_lambda': 71.0, 'max_depth': 2, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 2}. Best is trial 13 with value: 0.3725294493490526.\n","[I 2023-06-26 12:53:52,887] Trial 14 finished with value: 0.5211696173395567 and parameters: {'learning_rate': 0.28900000000000003, 'gamma': 1.041, 'reg_lambda': 51.0, 'max_depth': 5, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 2}. Best is trial 13 with value: 0.3725294493490526.\n","[I 2023-06-26 12:53:54,654] Trial 15 finished with value: 0.3494759650009102 and parameters: {'learning_rate': 0.329, 'gamma': 1.656, 'reg_lambda': 81.0, 'max_depth': 2, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 3}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:53:59,584] Trial 16 finished with value: 0.4385345672599663 and parameters: {'learning_rate': 0.325, 'gamma': 1.956, 'reg_lambda': 51.0, 'max_depth': 3, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 4}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:05,629] Trial 17 finished with value: 0.6295119049709383 and parameters: {'learning_rate': 0.369, 'gamma': 1.6909999999999998, 'reg_lambda': 81.0, 'max_depth': 5, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 3}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:06,846] Trial 18 finished with value: 0.5434866700529226 and parameters: {'learning_rate': 0.493, 'gamma': 1.0459999999999998, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.5, 'max_delta_step': 3}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:13,956] Trial 19 finished with value: 0.45144358453328515 and parameters: {'learning_rate': 0.273, 'gamma': 1.5859999999999999, 'reg_lambda': 81.0, 'max_depth': 5, 'min_child_weight': 0.4, 'max_delta_step': 4}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:21,076] Trial 20 finished with value: 0.8303529140117119 and parameters: {'learning_rate': 0.365, 'gamma': 0.756, 'reg_lambda': 41.0, 'max_depth': 10, 'min_child_weight': 0.2, 'max_delta_step': 3}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:22,861] Trial 21 finished with value: 0.38715832517524895 and parameters: {'learning_rate': 0.20900000000000002, 'gamma': 1.226, 'reg_lambda': 81.0, 'max_depth': 2, 'min_child_weight': 0.2, 'max_delta_step': 2}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:24,580] Trial 22 finished with value: 0.38874279230698167 and parameters: {'learning_rate': 0.23700000000000002, 'gamma': 1.506, 'reg_lambda': 81.0, 'max_depth': 2, 'min_child_weight': 0.2, 'max_delta_step': 2}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:27,202] Trial 23 finished with value: 0.3524875850119669 and parameters: {'learning_rate': 0.165, 'gamma': 1.1609999999999998, 'reg_lambda': 71.0, 'max_depth': 3, 'min_child_weight': 0.4, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:28,790] Trial 24 finished with value: 0.37094292206192514 and parameters: {'learning_rate': 0.149, 'gamma': 1.1809999999999998, 'reg_lambda': 71.0, 'max_depth': 3, 'min_child_weight': 0.5, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:31,181] Trial 25 finished with value: 0.3711471940664953 and parameters: {'learning_rate': 0.153, 'gamma': 0.581, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.5, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:35,527] Trial 26 finished with value: 0.36633070771150356 and parameters: {'learning_rate': 0.149, 'gamma': 1.0859999999999999, 'reg_lambda': 51.0, 'max_depth': 4, 'min_child_weight': 0.5, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:38,192] Trial 27 finished with value: 0.37311931488707917 and parameters: {'learning_rate': 0.121, 'gamma': 0.836, 'reg_lambda': 21.0, 'max_depth': 4, 'min_child_weight': 0.4, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:42,021] Trial 28 finished with value: 0.449419091975858 and parameters: {'learning_rate': 0.069, 'gamma': 1.091, 'reg_lambda': 51.0, 'max_depth': 6, 'min_child_weight': 0.6, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:44,083] Trial 29 finished with value: 0.36127575388040256 and parameters: {'learning_rate': 0.17300000000000001, 'gamma': 0.586, 'reg_lambda': 31.0, 'max_depth': 4, 'min_child_weight': 0.6, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:46,504] Trial 30 finished with value: 0.657026239368716 and parameters: {'learning_rate': 0.301, 'gamma': 0.401, 'reg_lambda': 21.0, 'max_depth': 5, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:48,865] Trial 31 finished with value: 0.40516284036004935 and parameters: {'learning_rate': 0.181, 'gamma': 0.591, 'reg_lambda': 41.0, 'max_depth': 4, 'min_child_weight': 0.6, 'max_delta_step': 4}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:50,556] Trial 32 finished with value: 0.4170394320640175 and parameters: {'learning_rate': 0.093, 'gamma': 0.331, 'reg_lambda': 31.0, 'max_depth': 3, 'min_child_weight': 0.4, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:54,495] Trial 33 finished with value: 0.441656239916204 and parameters: {'learning_rate': 0.181, 'gamma': 0.5710000000000001, 'reg_lambda': 31.0, 'max_depth': 6, 'min_child_weight': 0.5, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:56,942] Trial 34 finished with value: 0.5741629427813434 and parameters: {'learning_rate': 0.037000000000000005, 'gamma': 0.896, 'reg_lambda': 31.0, 'max_depth': 4, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 4}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:54:59,085] Trial 35 finished with value: 0.4462489437366979 and parameters: {'learning_rate': 0.077, 'gamma': 1.9609999999999999, 'reg_lambda': 11.0, 'max_depth': 3, 'min_child_weight': 0.6, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:00,488] Trial 36 finished with value: 0.45581460033725574 and parameters: {'learning_rate': 0.121, 'gamma': 0.401, 'reg_lambda': 91.0, 'max_depth': 2, 'min_child_weight': 0.4, 'max_delta_step': 3}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:02,918] Trial 37 finished with value: 0.6695963525946232 and parameters: {'learning_rate': 0.013000000000000001, 'gamma': 0.266, 'reg_lambda': 61.0, 'max_depth': 4, 'min_child_weight': 0.5, 'max_delta_step': 1}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:05,430] Trial 38 finished with value: 0.5775840940116842 and parameters: {'learning_rate': 0.329, 'gamma': 0.746, 'reg_lambda': 51.0, 'max_depth': 5, 'min_child_weight': 0.6, 'max_delta_step': 3}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:07,151] Trial 39 finished with value: 0.3629999493771603 and parameters: {'learning_rate': 0.157, 'gamma': 1.4109999999999998, 'reg_lambda': 41.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:09,106] Trial 40 finished with value: 0.37843139348154137 and parameters: {'learning_rate': 0.393, 'gamma': 0.021, 'reg_lambda': 41.0, 'max_depth': 3, 'min_child_weight': 0.9, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:10,627] Trial 41 finished with value: 0.35642199953137715 and parameters: {'learning_rate': 0.157, 'gamma': 1.146, 'reg_lambda': 31.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:12,095] Trial 42 finished with value: 0.4507444225073436 and parameters: {'learning_rate': 0.121, 'gamma': 1.436, 'reg_lambda': 31.0, 'max_depth': 2, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 15 with value: 0.3494759650009102.\n","[I 2023-06-26 12:55:13,805] Trial 43 finished with value: 0.3385919054120763 and parameters: {'learning_rate': 0.17300000000000001, 'gamma': 1.726, 'reg_lambda': 11.0, 'max_depth': 3, 'min_child_weight': 0.8, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:15,745] Trial 44 finished with value: 0.38336038616043155 and parameters: {'learning_rate': 0.225, 'gamma': 1.811, 'reg_lambda': 1.0, 'max_depth': 2, 'min_child_weight': 0.8, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:17,613] Trial 45 finished with value: 0.37482709781121765 and parameters: {'learning_rate': 0.185, 'gamma': 1.746, 'reg_lambda': 11.0, 'max_depth': 3, 'min_child_weight': 0.8, 'max_delta_step': 4}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:21,077] Trial 46 finished with value: 0.7673974675458333 and parameters: {'learning_rate': 0.253, 'gamma': 1.651, 'reg_lambda': 11.0, 'max_depth': 9, 'min_child_weight': 0.9, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:22,564] Trial 47 finished with value: 0.38194105878894447 and parameters: {'learning_rate': 0.197, 'gamma': 1.5559999999999998, 'reg_lambda': 21.0, 'max_depth': 2, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 4}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:24,563] Trial 48 finished with value: 0.7958419977186535 and parameters: {'learning_rate': 0.301, 'gamma': 1.8559999999999999, 'reg_lambda': 1.0, 'max_depth': 4, 'min_child_weight': 0.9, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:26,123] Trial 49 finished with value: 0.4301506316908778 and parameters: {'learning_rate': 0.273, 'gamma': 1.3259999999999998, 'reg_lambda': 21.0, 'max_depth': 3, 'min_child_weight': 0.8, 'max_delta_step': 4}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:27,992] Trial 50 finished with value: 0.3965790703903518 and parameters: {'learning_rate': 0.169, 'gamma': 1.1609999999999998, 'reg_lambda': 21.0, 'max_depth': 2, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:30,053] Trial 51 finished with value: 0.36821236469328583 and parameters: {'learning_rate': 0.129, 'gamma': 0.9510000000000001, 'reg_lambda': 41.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:31,876] Trial 52 finished with value: 0.35289058361883435 and parameters: {'learning_rate': 0.165, 'gamma': 1.436, 'reg_lambda': 91.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:34,008] Trial 53 finished with value: 0.37191417862585857 and parameters: {'learning_rate': 0.217, 'gamma': 1.611, 'reg_lambda': 91.0, 'max_depth': 4, 'min_child_weight': 0.6, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:35,459] Trial 54 finished with value: 0.4082104476834272 and parameters: {'learning_rate': 0.169, 'gamma': 1.4909999999999999, 'reg_lambda': 91.0, 'max_depth': 2, 'min_child_weight': 0.8, 'max_delta_step': 4}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:37,005] Trial 55 finished with value: 0.4382418446386702 and parameters: {'learning_rate': 0.093, 'gamma': 1.3359999999999999, 'reg_lambda': 81.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:39,407] Trial 56 finished with value: 0.3924199827318974 and parameters: {'learning_rate': 0.133, 'gamma': 1.7009999999999998, 'reg_lambda': 91.0, 'max_depth': 7, 'min_child_weight': 0.6, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:41,109] Trial 57 finished with value: 0.4253503119279677 and parameters: {'learning_rate': 0.109, 'gamma': 1.166, 'reg_lambda': 81.0, 'max_depth': 3, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:42,665] Trial 58 finished with value: 0.38082036833355165 and parameters: {'learning_rate': 0.23700000000000002, 'gamma': 1.886, 'reg_lambda': 71.0, 'max_depth': 2, 'min_child_weight': 0.2, 'max_delta_step': 4}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:44,398] Trial 59 finished with value: 0.47130996756875765 and parameters: {'learning_rate': 0.201, 'gamma': 1.541, 'reg_lambda': 11.0, 'max_depth': 4, 'min_child_weight': 0.4, 'max_delta_step': 2}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:46,802] Trial 60 finished with value: 0.5416362896120452 and parameters: {'learning_rate': 0.049, 'gamma': 1.251, 'reg_lambda': 81.0, 'max_depth': 5, 'min_child_weight': 0.8, 'max_delta_step': 3}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:48,522] Trial 61 finished with value: 0.37408938009374343 and parameters: {'learning_rate': 0.161, 'gamma': 1.4309999999999998, 'reg_lambda': 41.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:50,207] Trial 62 finished with value: 0.3578042796391018 and parameters: {'learning_rate': 0.14100000000000001, 'gamma': 1.361, 'reg_lambda': 31.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:51,823] Trial 63 finished with value: 0.38643817518443757 and parameters: {'learning_rate': 0.14500000000000002, 'gamma': 1.331, 'reg_lambda': 31.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:53,077] Trial 64 finished with value: 0.47319610490286595 and parameters: {'learning_rate': 0.10500000000000001, 'gamma': 1.476, 'reg_lambda': 71.0, 'max_depth': 2, 'min_child_weight': 0.8, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:56,311] Trial 65 finished with value: 0.4209970522750498 and parameters: {'learning_rate': 0.197, 'gamma': 0.981, 'reg_lambda': 31.0, 'max_depth': 8, 'min_child_weight': 0.6, 'max_delta_step': 5}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:58,291] Trial 66 finished with value: 0.5257056512945268 and parameters: {'learning_rate': 0.453, 'gamma': 1.126, 'reg_lambda': 91.0, 'max_depth': 4, 'min_child_weight': 0.8, 'max_delta_step': 4}. Best is trial 43 with value: 0.3385919054120763.\n","[I 2023-06-26 12:55:59,744] Trial 67 finished with value: 0.3255559405627576 and parameters: {'learning_rate': 0.225, 'gamma': 1.2309999999999999, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:01,488] Trial 68 finished with value: 0.3310847774173152 and parameters: {'learning_rate': 0.269, 'gamma': 1.226, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:03,051] Trial 69 finished with value: 0.3600007095027288 and parameters: {'learning_rate': 0.28500000000000003, 'gamma': 1.216, 'reg_lambda': 61.0, 'max_depth': 2, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:04,535] Trial 70 finished with value: 0.3811014279354698 and parameters: {'learning_rate': 0.353, 'gamma': 1.0359999999999998, 'reg_lambda': 71.0, 'max_depth': 3, 'min_child_weight': 0.2, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:06,065] Trial 71 finished with value: 0.36623918284355933 and parameters: {'learning_rate': 0.221, 'gamma': 1.376, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:07,656] Trial 72 finished with value: 0.36356433371519964 and parameters: {'learning_rate': 0.269, 'gamma': 1.276, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.4, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:09,680] Trial 73 finished with value: 0.4216657256772084 and parameters: {'learning_rate': 0.321, 'gamma': 1.081, 'reg_lambda': 71.0, 'max_depth': 4, 'min_child_weight': 0.7000000000000001, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:11,629] Trial 74 finished with value: 0.4139648058707054 and parameters: {'learning_rate': 0.185, 'gamma': 1.636, 'reg_lambda': 51.0, 'max_depth': 6, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:12,819] Trial 75 finished with value: 0.38469743593652106 and parameters: {'learning_rate': 0.229, 'gamma': 1.226, 'reg_lambda': 81.0, 'max_depth': 2, 'min_child_weight': 0.4, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:14,690] Trial 76 finished with value: 0.5108581048734365 and parameters: {'learning_rate': 0.393, 'gamma': 1.3659999999999999, 'reg_lambda': 71.0, 'max_depth': 3, 'min_child_weight': 0.2, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:16,359] Trial 77 finished with value: 0.3469480354144624 and parameters: {'learning_rate': 0.253, 'gamma': 0.901, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:18,014] Trial 78 finished with value: 0.40308802466761795 and parameters: {'learning_rate': 0.253, 'gamma': 0.881, 'reg_lambda': 61.0, 'max_depth': 4, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:19,230] Trial 79 finished with value: 0.3535107828917693 and parameters: {'learning_rate': 0.313, 'gamma': 1.0259999999999998, 'reg_lambda': 61.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:20,627] Trial 80 finished with value: 0.3781389036994852 and parameters: {'learning_rate': 0.309, 'gamma': 0.791, 'reg_lambda': 61.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:22,049] Trial 81 finished with value: 0.3500448617848072 and parameters: {'learning_rate': 0.28900000000000003, 'gamma': 1.006, 'reg_lambda': 51.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:23,333] Trial 82 finished with value: 0.36021509939814295 and parameters: {'learning_rate': 0.293, 'gamma': 0.676, 'reg_lambda': 51.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:24,475] Trial 83 finished with value: 0.36614059046238573 and parameters: {'learning_rate': 0.265, 'gamma': 1.021, 'reg_lambda': 51.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:25,769] Trial 84 finished with value: 0.3867410827787462 and parameters: {'learning_rate': 0.333, 'gamma': 0.986, 'reg_lambda': 61.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:26,964] Trial 85 finished with value: 0.3571864572770156 and parameters: {'learning_rate': 0.313, 'gamma': 0.871, 'reg_lambda': 71.0, 'max_depth': 2, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:28,632] Trial 86 finished with value: 0.4001007914784612 and parameters: {'learning_rate': 0.341, 'gamma': 1.101, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:29,989] Trial 87 finished with value: 0.3710024989971075 and parameters: {'learning_rate': 0.281, 'gamma': 1.2009999999999998, 'reg_lambda': 51.0, 'max_depth': 2, 'min_child_weight': 0.8, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:31,524] Trial 88 finished with value: 0.3905255705736513 and parameters: {'learning_rate': 0.23700000000000002, 'gamma': 1.756, 'reg_lambda': 61.0, 'max_depth': 3, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:33,034] Trial 89 finished with value: 0.36782593570351435 and parameters: {'learning_rate': 0.245, 'gamma': 0.9510000000000001, 'reg_lambda': 51.0, 'max_depth': 2, 'min_child_weight': 0.8, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:34,784] Trial 90 finished with value: 0.37770453480623023 and parameters: {'learning_rate': 0.373, 'gamma': 0.931, 'reg_lambda': 71.0, 'max_depth': 3, 'min_child_weight': 0.8, 'max_delta_step': 2}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:37,614] Trial 91 finished with value: 0.4465599843091702 and parameters: {'learning_rate': 0.20900000000000002, 'gamma': 1.136, 'reg_lambda': 51.0, 'max_depth': 9, 'min_child_weight': 0.9, 'max_delta_step': 3}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:39,434] Trial 92 finished with value: 0.3431497528334409 and parameters: {'learning_rate': 0.257, 'gamma': 1.071, 'reg_lambda': 71.0, 'max_depth': 3, 'min_child_weight': 0.8, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:41,140] Trial 93 finished with value: 0.38264063304941043 and parameters: {'learning_rate': 0.297, 'gamma': 0.8160000000000001, 'reg_lambda': 71.0, 'max_depth': 3, 'min_child_weight': 0.8, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:43,545] Trial 94 finished with value: 0.3473051192333413 and parameters: {'learning_rate': 0.261, 'gamma': 1.0559999999999998, 'reg_lambda': 81.0, 'max_depth': 3, 'min_child_weight': 0.9, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:45,733] Trial 95 finished with value: 0.39404658209745064 and parameters: {'learning_rate': 0.261, 'gamma': 1.801, 'reg_lambda': 81.0, 'max_depth': 4, 'min_child_weight': 0.4, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:47,611] Trial 96 finished with value: 0.3630715338877032 and parameters: {'learning_rate': 0.257, 'gamma': 1.0859999999999999, 'reg_lambda': 81.0, 'max_depth': 3, 'min_child_weight': 0.5, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:49,797] Trial 97 finished with value: 0.3654604895340328 and parameters: {'learning_rate': 0.277, 'gamma': 1.686, 'reg_lambda': 91.0, 'max_depth': 3, 'min_child_weight': 0.8, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:52,084] Trial 98 finished with value: 0.4229881032769588 and parameters: {'learning_rate': 0.233, 'gamma': 1.276, 'reg_lambda': 71.0, 'max_depth': 4, 'min_child_weight': 0.30000000000000004, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n","[I 2023-06-26 12:56:54,076] Trial 99 finished with value: 0.32694649044537194 and parameters: {'learning_rate': 0.249, 'gamma': 1.0659999999999998, 'reg_lambda': 81.0, 'max_depth': 3, 'min_child_weight': 0.9, 'max_delta_step': 1}. Best is trial 67 with value: 0.3255559405627576.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>learning_rate</th>\n","      <th>gamma</th>\n","      <th>reg_lambda</th>\n","      <th>max_depth</th>\n","      <th>min_child_weight</th>\n","      <th>max_delta_step</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>67</th>\n","      <td>0.225</td>\n","      <td>1.231</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>2</td>\n","      <td>0.325556</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>0.249</td>\n","      <td>1.066</td>\n","      <td>81.0</td>\n","      <td>3</td>\n","      <td>0.9</td>\n","      <td>1</td>\n","      <td>0.326946</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>0.269</td>\n","      <td>1.226</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.3</td>\n","      <td>2</td>\n","      <td>0.331085</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>0.173</td>\n","      <td>1.726</td>\n","      <td>11.0</td>\n","      <td>3</td>\n","      <td>0.8</td>\n","      <td>5</td>\n","      <td>0.338592</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>0.257</td>\n","      <td>1.071</td>\n","      <td>71.0</td>\n","      <td>3</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>0.343150</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>0.253</td>\n","      <td>0.901</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.346948</td>\n","    </tr>\n","    <tr>\n","      <th>94</th>\n","      <td>0.261</td>\n","      <td>1.056</td>\n","      <td>81.0</td>\n","      <td>3</td>\n","      <td>0.9</td>\n","      <td>1</td>\n","      <td>0.347305</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.329</td>\n","      <td>1.656</td>\n","      <td>81.0</td>\n","      <td>2</td>\n","      <td>0.3</td>\n","      <td>3</td>\n","      <td>0.349476</td>\n","    </tr>\n","    <tr>\n","      <th>81</th>\n","      <td>0.289</td>\n","      <td>1.006</td>\n","      <td>51.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.350045</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.165</td>\n","      <td>1.161</td>\n","      <td>71.0</td>\n","      <td>3</td>\n","      <td>0.4</td>\n","      <td>1</td>\n","      <td>0.352488</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>0.165</td>\n","      <td>1.436</td>\n","      <td>91.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.352891</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>0.313</td>\n","      <td>1.026</td>\n","      <td>61.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.353511</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>0.157</td>\n","      <td>1.146</td>\n","      <td>31.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.356422</td>\n","    </tr>\n","    <tr>\n","      <th>85</th>\n","      <td>0.313</td>\n","      <td>0.871</td>\n","      <td>71.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.357186</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>0.141</td>\n","      <td>1.361</td>\n","      <td>31.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.357804</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>0.285</td>\n","      <td>1.216</td>\n","      <td>61.0</td>\n","      <td>2</td>\n","      <td>0.3</td>\n","      <td>2</td>\n","      <td>0.360001</td>\n","    </tr>\n","    <tr>\n","      <th>82</th>\n","      <td>0.293</td>\n","      <td>0.676</td>\n","      <td>51.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.360215</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0.173</td>\n","      <td>0.586</td>\n","      <td>31.0</td>\n","      <td>4</td>\n","      <td>0.6</td>\n","      <td>5</td>\n","      <td>0.361276</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>0.157</td>\n","      <td>1.411</td>\n","      <td>41.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.363000</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>0.257</td>\n","      <td>1.086</td>\n","      <td>81.0</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1</td>\n","      <td>0.363072</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>0.269</td>\n","      <td>1.276</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.4</td>\n","      <td>3</td>\n","      <td>0.363564</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>0.277</td>\n","      <td>1.686</td>\n","      <td>91.0</td>\n","      <td>3</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>0.365460</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>0.265</td>\n","      <td>1.021</td>\n","      <td>51.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.366141</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>0.221</td>\n","      <td>1.376</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.3</td>\n","      <td>2</td>\n","      <td>0.366239</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0.149</td>\n","      <td>1.086</td>\n","      <td>51.0</td>\n","      <td>4</td>\n","      <td>0.5</td>\n","      <td>1</td>\n","      <td>0.366331</td>\n","    </tr>\n","    <tr>\n","      <th>89</th>\n","      <td>0.245</td>\n","      <td>0.951</td>\n","      <td>51.0</td>\n","      <td>2</td>\n","      <td>0.8</td>\n","      <td>2</td>\n","      <td>0.367826</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>0.129</td>\n","      <td>0.951</td>\n","      <td>41.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.368212</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.149</td>\n","      <td>1.181</td>\n","      <td>71.0</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1</td>\n","      <td>0.370943</td>\n","    </tr>\n","    <tr>\n","      <th>87</th>\n","      <td>0.281</td>\n","      <td>1.201</td>\n","      <td>51.0</td>\n","      <td>2</td>\n","      <td>0.8</td>\n","      <td>3</td>\n","      <td>0.371002</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0.153</td>\n","      <td>0.581</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1</td>\n","      <td>0.371147</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>0.217</td>\n","      <td>1.611</td>\n","      <td>91.0</td>\n","      <td>4</td>\n","      <td>0.6</td>\n","      <td>5</td>\n","      <td>0.371914</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.241</td>\n","      <td>1.276</td>\n","      <td>71.0</td>\n","      <td>2</td>\n","      <td>0.3</td>\n","      <td>2</td>\n","      <td>0.372529</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.213</td>\n","      <td>1.396</td>\n","      <td>81.0</td>\n","      <td>2</td>\n","      <td>0.1</td>\n","      <td>2</td>\n","      <td>0.373113</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0.121</td>\n","      <td>0.836</td>\n","      <td>21.0</td>\n","      <td>4</td>\n","      <td>0.4</td>\n","      <td>1</td>\n","      <td>0.373119</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.181</td>\n","      <td>1.401</td>\n","      <td>71.0</td>\n","      <td>2</td>\n","      <td>0.1</td>\n","      <td>2</td>\n","      <td>0.373884</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>0.161</td>\n","      <td>1.431</td>\n","      <td>41.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.374089</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>0.185</td>\n","      <td>1.746</td>\n","      <td>11.0</td>\n","      <td>3</td>\n","      <td>0.8</td>\n","      <td>4</td>\n","      <td>0.374827</td>\n","    </tr>\n","    <tr>\n","      <th>90</th>\n","      <td>0.373</td>\n","      <td>0.931</td>\n","      <td>71.0</td>\n","      <td>3</td>\n","      <td>0.8</td>\n","      <td>2</td>\n","      <td>0.377705</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>0.309</td>\n","      <td>0.791</td>\n","      <td>61.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.378139</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>0.393</td>\n","      <td>0.021</td>\n","      <td>41.0</td>\n","      <td>3</td>\n","      <td>0.9</td>\n","      <td>5</td>\n","      <td>0.378431</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>0.237</td>\n","      <td>1.886</td>\n","      <td>71.0</td>\n","      <td>2</td>\n","      <td>0.2</td>\n","      <td>4</td>\n","      <td>0.380820</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>0.353</td>\n","      <td>1.036</td>\n","      <td>71.0</td>\n","      <td>3</td>\n","      <td>0.2</td>\n","      <td>2</td>\n","      <td>0.381101</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>0.197</td>\n","      <td>1.556</td>\n","      <td>21.0</td>\n","      <td>2</td>\n","      <td>0.7</td>\n","      <td>4</td>\n","      <td>0.381941</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>0.297</td>\n","      <td>0.816</td>\n","      <td>71.0</td>\n","      <td>3</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>0.382641</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>0.225</td>\n","      <td>1.811</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>0.8</td>\n","      <td>5</td>\n","      <td>0.383360</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>0.229</td>\n","      <td>1.226</td>\n","      <td>81.0</td>\n","      <td>2</td>\n","      <td>0.4</td>\n","      <td>2</td>\n","      <td>0.384697</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>0.145</td>\n","      <td>1.331</td>\n","      <td>31.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.386438</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>0.333</td>\n","      <td>0.986</td>\n","      <td>61.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.386741</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.209</td>\n","      <td>1.226</td>\n","      <td>81.0</td>\n","      <td>2</td>\n","      <td>0.2</td>\n","      <td>2</td>\n","      <td>0.387158</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0.237</td>\n","      <td>1.506</td>\n","      <td>81.0</td>\n","      <td>2</td>\n","      <td>0.2</td>\n","      <td>2</td>\n","      <td>0.388743</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>0.237</td>\n","      <td>1.756</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.390526</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>0.133</td>\n","      <td>1.701</td>\n","      <td>91.0</td>\n","      <td>7</td>\n","      <td>0.6</td>\n","      <td>5</td>\n","      <td>0.392420</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.141</td>\n","      <td>1.506</td>\n","      <td>91.0</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>2</td>\n","      <td>0.392615</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>0.261</td>\n","      <td>1.801</td>\n","      <td>81.0</td>\n","      <td>4</td>\n","      <td>0.4</td>\n","      <td>1</td>\n","      <td>0.394047</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>0.169</td>\n","      <td>1.161</td>\n","      <td>21.0</td>\n","      <td>2</td>\n","      <td>0.3</td>\n","      <td>5</td>\n","      <td>0.396579</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>0.341</td>\n","      <td>1.101</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.400101</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>0.253</td>\n","      <td>0.881</td>\n","      <td>61.0</td>\n","      <td>4</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.403088</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>0.181</td>\n","      <td>0.591</td>\n","      <td>41.0</td>\n","      <td>4</td>\n","      <td>0.6</td>\n","      <td>4</td>\n","      <td>0.405163</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.105</td>\n","      <td>0.741</td>\n","      <td>91.0</td>\n","      <td>4</td>\n","      <td>0.8</td>\n","      <td>4</td>\n","      <td>0.406164</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>0.169</td>\n","      <td>1.491</td>\n","      <td>91.0</td>\n","      <td>2</td>\n","      <td>0.8</td>\n","      <td>4</td>\n","      <td>0.408210</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>0.185</td>\n","      <td>1.636</td>\n","      <td>51.0</td>\n","      <td>6</td>\n","      <td>0.3</td>\n","      <td>1</td>\n","      <td>0.413965</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>0.093</td>\n","      <td>0.331</td>\n","      <td>31.0</td>\n","      <td>3</td>\n","      <td>0.4</td>\n","      <td>5</td>\n","      <td>0.417039</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.205</td>\n","      <td>1.291</td>\n","      <td>71.0</td>\n","      <td>10</td>\n","      <td>0.1</td>\n","      <td>2</td>\n","      <td>0.419458</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>0.197</td>\n","      <td>0.981</td>\n","      <td>31.0</td>\n","      <td>8</td>\n","      <td>0.6</td>\n","      <td>5</td>\n","      <td>0.420997</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>0.321</td>\n","      <td>1.081</td>\n","      <td>71.0</td>\n","      <td>4</td>\n","      <td>0.7</td>\n","      <td>3</td>\n","      <td>0.421666</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>0.233</td>\n","      <td>1.276</td>\n","      <td>71.0</td>\n","      <td>4</td>\n","      <td>0.3</td>\n","      <td>1</td>\n","      <td>0.422988</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>0.109</td>\n","      <td>1.166</td>\n","      <td>81.0</td>\n","      <td>3</td>\n","      <td>0.3</td>\n","      <td>5</td>\n","      <td>0.425350</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0.273</td>\n","      <td>1.326</td>\n","      <td>21.0</td>\n","      <td>3</td>\n","      <td>0.8</td>\n","      <td>4</td>\n","      <td>0.430151</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>0.093</td>\n","      <td>1.336</td>\n","      <td>81.0</td>\n","      <td>3</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.438242</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.325</td>\n","      <td>1.956</td>\n","      <td>51.0</td>\n","      <td>3</td>\n","      <td>0.3</td>\n","      <td>4</td>\n","      <td>0.438535</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>0.181</td>\n","      <td>0.571</td>\n","      <td>31.0</td>\n","      <td>6</td>\n","      <td>0.5</td>\n","      <td>1</td>\n","      <td>0.441656</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>0.077</td>\n","      <td>1.961</td>\n","      <td>11.0</td>\n","      <td>3</td>\n","      <td>0.6</td>\n","      <td>5</td>\n","      <td>0.446249</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>0.209</td>\n","      <td>1.136</td>\n","      <td>51.0</td>\n","      <td>9</td>\n","      <td>0.9</td>\n","      <td>3</td>\n","      <td>0.446560</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0.069</td>\n","      <td>1.091</td>\n","      <td>51.0</td>\n","      <td>6</td>\n","      <td>0.6</td>\n","      <td>1</td>\n","      <td>0.449419</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>0.121</td>\n","      <td>1.436</td>\n","      <td>31.0</td>\n","      <td>2</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.450744</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.273</td>\n","      <td>1.586</td>\n","      <td>81.0</td>\n","      <td>5</td>\n","      <td>0.4</td>\n","      <td>4</td>\n","      <td>0.451444</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>0.121</td>\n","      <td>0.401</td>\n","      <td>91.0</td>\n","      <td>2</td>\n","      <td>0.4</td>\n","      <td>3</td>\n","      <td>0.455815</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>0.201</td>\n","      <td>1.541</td>\n","      <td>11.0</td>\n","      <td>4</td>\n","      <td>0.4</td>\n","      <td>2</td>\n","      <td>0.471310</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>0.105</td>\n","      <td>1.476</td>\n","      <td>71.0</td>\n","      <td>2</td>\n","      <td>0.8</td>\n","      <td>5</td>\n","      <td>0.473196</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.089</td>\n","      <td>0.401</td>\n","      <td>41.0</td>\n","      <td>2</td>\n","      <td>0.9</td>\n","      <td>4</td>\n","      <td>0.487046</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>0.393</td>\n","      <td>1.366</td>\n","      <td>71.0</td>\n","      <td>3</td>\n","      <td>0.2</td>\n","      <td>2</td>\n","      <td>0.510858</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.289</td>\n","      <td>1.041</td>\n","      <td>51.0</td>\n","      <td>5</td>\n","      <td>0.3</td>\n","      <td>2</td>\n","      <td>0.521170</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>0.453</td>\n","      <td>1.126</td>\n","      <td>91.0</td>\n","      <td>4</td>\n","      <td>0.8</td>\n","      <td>4</td>\n","      <td>0.525706</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.053</td>\n","      <td>1.846</td>\n","      <td>61.0</td>\n","      <td>7</td>\n","      <td>0.7</td>\n","      <td>3</td>\n","      <td>0.527262</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>0.049</td>\n","      <td>1.251</td>\n","      <td>81.0</td>\n","      <td>5</td>\n","      <td>0.8</td>\n","      <td>3</td>\n","      <td>0.541636</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.493</td>\n","      <td>1.046</td>\n","      <td>61.0</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>3</td>\n","      <td>0.543487</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>0.037</td>\n","      <td>0.896</td>\n","      <td>31.0</td>\n","      <td>4</td>\n","      <td>0.7</td>\n","      <td>4</td>\n","      <td>0.574163</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>0.329</td>\n","      <td>0.746</td>\n","      <td>51.0</td>\n","      <td>5</td>\n","      <td>0.6</td>\n","      <td>3</td>\n","      <td>0.577584</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.341</td>\n","      <td>0.206</td>\n","      <td>91.0</td>\n","      <td>8</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>0.620691</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.369</td>\n","      <td>1.691</td>\n","      <td>81.0</td>\n","      <td>5</td>\n","      <td>0.3</td>\n","      <td>3</td>\n","      <td>0.629512</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.257</td>\n","      <td>1.281</td>\n","      <td>21.0</td>\n","      <td>8</td>\n","      <td>0.4</td>\n","      <td>3</td>\n","      <td>0.642881</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.017</td>\n","      <td>0.151</td>\n","      <td>31.0</td>\n","      <td>4</td>\n","      <td>0.9</td>\n","      <td>5</td>\n","      <td>0.648500</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0.301</td>\n","      <td>0.401</td>\n","      <td>21.0</td>\n","      <td>5</td>\n","      <td>0.7</td>\n","      <td>5</td>\n","      <td>0.657026</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>0.013</td>\n","      <td>0.266</td>\n","      <td>61.0</td>\n","      <td>4</td>\n","      <td>0.5</td>\n","      <td>1</td>\n","      <td>0.669596</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.421</td>\n","      <td>0.106</td>\n","      <td>61.0</td>\n","      <td>7</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>0.747582</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.445</td>\n","      <td>0.921</td>\n","      <td>71.0</td>\n","      <td>7</td>\n","      <td>0.6</td>\n","      <td>5</td>\n","      <td>0.762273</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>0.253</td>\n","      <td>1.651</td>\n","      <td>11.0</td>\n","      <td>9</td>\n","      <td>0.9</td>\n","      <td>5</td>\n","      <td>0.767397</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>0.301</td>\n","      <td>1.856</td>\n","      <td>1.0</td>\n","      <td>4</td>\n","      <td>0.9</td>\n","      <td>5</td>\n","      <td>0.795842</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.365</td>\n","      <td>0.756</td>\n","      <td>41.0</td>\n","      <td>10</td>\n","      <td>0.2</td>\n","      <td>3</td>\n","      <td>0.830353</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.429</td>\n","      <td>1.756</td>\n","      <td>1.0</td>\n","      <td>4</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>1.002167</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    learning_rate  gamma  reg_lambda  max_depth  min_child_weight  \\\n","67          0.225  1.231        61.0          3               0.7   \n","99          0.249  1.066        81.0          3               0.9   \n","68          0.269  1.226        61.0          3               0.3   \n","43          0.173  1.726        11.0          3               0.8   \n","92          0.257  1.071        71.0          3               0.8   \n","77          0.253  0.901        61.0          3               0.9   \n","94          0.261  1.056        81.0          3               0.9   \n","15          0.329  1.656        81.0          2               0.3   \n","81          0.289  1.006        51.0          2               0.9   \n","23          0.165  1.161        71.0          3               0.4   \n","52          0.165  1.436        91.0          3               0.7   \n","79          0.313  1.026        61.0          2               0.9   \n","41          0.157  1.146        31.0          3               0.7   \n","85          0.313  0.871        71.0          2               0.9   \n","62          0.141  1.361        31.0          3               0.7   \n","69          0.285  1.216        61.0          2               0.3   \n","82          0.293  0.676        51.0          2               0.9   \n","29          0.173  0.586        31.0          4               0.6   \n","39          0.157  1.411        41.0          3               0.7   \n","96          0.257  1.086        81.0          3               0.5   \n","72          0.269  1.276        61.0          3               0.4   \n","97          0.277  1.686        91.0          3               0.8   \n","83          0.265  1.021        51.0          2               0.9   \n","71          0.221  1.376        61.0          3               0.3   \n","26          0.149  1.086        51.0          4               0.5   \n","89          0.245  0.951        51.0          2               0.8   \n","51          0.129  0.951        41.0          3               0.7   \n","24          0.149  1.181        71.0          3               0.5   \n","87          0.281  1.201        51.0          2               0.8   \n","25          0.153  0.581        61.0          3               0.5   \n","53          0.217  1.611        91.0          4               0.6   \n","13          0.241  1.276        71.0          2               0.3   \n","11          0.213  1.396        81.0          2               0.1   \n","27          0.121  0.836        21.0          4               0.4   \n","10          0.181  1.401        71.0          2               0.1   \n","61          0.161  1.431        41.0          3               0.7   \n","45          0.185  1.746        11.0          3               0.8   \n","90          0.373  0.931        71.0          3               0.8   \n","80          0.309  0.791        61.0          2               0.9   \n","40          0.393  0.021        41.0          3               0.9   \n","58          0.237  1.886        71.0          2               0.2   \n","70          0.353  1.036        71.0          3               0.2   \n","47          0.197  1.556        21.0          2               0.7   \n","93          0.297  0.816        71.0          3               0.8   \n","44          0.225  1.811         1.0          2               0.8   \n","75          0.229  1.226        81.0          2               0.4   \n","63          0.145  1.331        31.0          3               0.7   \n","84          0.333  0.986        61.0          2               0.9   \n","21          0.209  1.226        81.0          2               0.2   \n","22          0.237  1.506        81.0          2               0.2   \n","88          0.237  1.756        61.0          3               0.9   \n","56          0.133  1.701        91.0          7               0.6   \n","4           0.141  1.506        91.0          3               0.1   \n","95          0.261  1.801        81.0          4               0.4   \n","50          0.169  1.161        21.0          2               0.3   \n","86          0.341  1.101        61.0          3               0.9   \n","78          0.253  0.881        61.0          4               0.9   \n","31          0.181  0.591        41.0          4               0.6   \n","8           0.105  0.741        91.0          4               0.8   \n","54          0.169  1.491        91.0          2               0.8   \n","74          0.185  1.636        51.0          6               0.3   \n","32          0.093  0.331        31.0          3               0.4   \n","12          0.205  1.291        71.0         10               0.1   \n","65          0.197  0.981        31.0          8               0.6   \n","73          0.321  1.081        71.0          4               0.7   \n","98          0.233  1.276        71.0          4               0.3   \n","57          0.109  1.166        81.0          3               0.3   \n","49          0.273  1.326        21.0          3               0.8   \n","55          0.093  1.336        81.0          3               0.7   \n","16          0.325  1.956        51.0          3               0.3   \n","33          0.181  0.571        31.0          6               0.5   \n","35          0.077  1.961        11.0          3               0.6   \n","91          0.209  1.136        51.0          9               0.9   \n","28          0.069  1.091        51.0          6               0.6   \n","42          0.121  1.436        31.0          2               0.7   \n","19          0.273  1.586        81.0          5               0.4   \n","36          0.121  0.401        91.0          2               0.4   \n","59          0.201  1.541        11.0          4               0.4   \n","64          0.105  1.476        71.0          2               0.8   \n","0           0.089  0.401        41.0          2               0.9   \n","76          0.393  1.366        71.0          3               0.2   \n","14          0.289  1.041        51.0          5               0.3   \n","66          0.453  1.126        91.0          4               0.8   \n","2           0.053  1.846        61.0          7               0.7   \n","60          0.049  1.251        81.0          5               0.8   \n","18          0.493  1.046        61.0          3               0.5   \n","34          0.037  0.896        31.0          4               0.7   \n","38          0.329  0.746        51.0          5               0.6   \n","1           0.341  0.206        91.0          8               0.8   \n","17          0.369  1.691        81.0          5               0.3   \n","7           0.257  1.281        21.0          8               0.4   \n","3           0.017  0.151        31.0          4               0.9   \n","30          0.301  0.401        21.0          5               0.7   \n","37          0.013  0.266        61.0          4               0.5   \n","5           0.421  0.106        61.0          7               0.8   \n","6           0.445  0.921        71.0          7               0.6   \n","46          0.253  1.651        11.0          9               0.9   \n","48          0.301  1.856         1.0          4               0.9   \n","20          0.365  0.756        41.0         10               0.2   \n","9           0.429  1.756         1.0          4               0.8   \n","\n","    max_delta_step     score  \n","67               2  0.325556  \n","99               1  0.326946  \n","68               2  0.331085  \n","43               5  0.338592  \n","92               1  0.343150  \n","77               3  0.346948  \n","94               1  0.347305  \n","15               3  0.349476  \n","81               3  0.350045  \n","23               1  0.352488  \n","52               5  0.352891  \n","79               3  0.353511  \n","41               5  0.356422  \n","85               3  0.357186  \n","62               5  0.357804  \n","69               2  0.360001  \n","82               3  0.360215  \n","29               5  0.361276  \n","39               5  0.363000  \n","96               1  0.363072  \n","72               3  0.363564  \n","97               1  0.365460  \n","83               3  0.366141  \n","71               2  0.366239  \n","26               1  0.366331  \n","89               2  0.367826  \n","51               5  0.368212  \n","24               1  0.370943  \n","87               3  0.371002  \n","25               1  0.371147  \n","53               5  0.371914  \n","13               2  0.372529  \n","11               2  0.373113  \n","27               1  0.373119  \n","10               2  0.373884  \n","61               5  0.374089  \n","45               4  0.374827  \n","90               2  0.377705  \n","80               3  0.378139  \n","40               5  0.378431  \n","58               4  0.380820  \n","70               2  0.381101  \n","47               4  0.381941  \n","93               1  0.382641  \n","44               5  0.383360  \n","75               2  0.384697  \n","63               5  0.386438  \n","84               3  0.386741  \n","21               2  0.387158  \n","22               2  0.388743  \n","88               3  0.390526  \n","56               5  0.392420  \n","4                2  0.392615  \n","95               1  0.394047  \n","50               5  0.396579  \n","86               3  0.400101  \n","78               3  0.403088  \n","31               4  0.405163  \n","8                4  0.406164  \n","54               4  0.408210  \n","74               1  0.413965  \n","32               5  0.417039  \n","12               2  0.419458  \n","65               5  0.420997  \n","73               3  0.421666  \n","98               1  0.422988  \n","57               5  0.425350  \n","49               4  0.430151  \n","55               5  0.438242  \n","16               4  0.438535  \n","33               1  0.441656  \n","35               5  0.446249  \n","91               3  0.446560  \n","28               1  0.449419  \n","42               5  0.450744  \n","19               4  0.451444  \n","36               3  0.455815  \n","59               2  0.471310  \n","64               5  0.473196  \n","0                4  0.487046  \n","76               2  0.510858  \n","14               2  0.521170  \n","66               4  0.525706  \n","2                3  0.527262  \n","60               3  0.541636  \n","18               3  0.543487  \n","34               4  0.574163  \n","38               3  0.577584  \n","1                1  0.620691  \n","17               3  0.629512  \n","7                3  0.642881  \n","3                5  0.648500  \n","30               5  0.657026  \n","37               1  0.669596  \n","5                1  0.747582  \n","6                5  0.762273  \n","46               5  0.767397  \n","48               5  0.795842  \n","20               3  0.830353  \n","9                1  1.002167  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["def objective(trial):\n","    optimized_param = {'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.5, step=0.004),\n","                       'gamma': trial.suggest_float('gamma', 1e-3, 2.0, step=0.005),\n","                       'reg_lambda': trial.suggest_float('reg_lambda', 1, 100, step=10),\n","                       # 'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1e-3, 10),\n","                       'max_depth': trial.suggest_int('max_depth', 2, 10),\n","                       'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 0.95, step=0.1),\n","                       'max_delta_step': trial.suggest_int('max_delta_step', 1, 5),\n","                        }\n","    \n","    # Perform 10-fold cross-validation\n","    kf = KFold(n_splits=10, shuffle=True)\n","    mean_balanced_logloss_score = []\n","\n","    for train_index, test_index in kf.split(X):\n","\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        sampler = RandomOverSampler()\n","        X_re, y_re = sampler.fit_resample(X_train, y_train)\n","        \n","        # Train a XGBoost model\n","        train_set = xgb.DMatrix(X_re, y_re)\n","        test_set = xgb.DMatrix(X_test, y_test)\n","        \n","        clf = xgb.train(params=optimized_param,\n","                        dtrain=train_set,\n","                        obj=balancedlogloss,\n","                        num_boost_round=40,\n","                        )\n","\n","        # Make predictions on the test set\n","        preds = clf.predict(xgb.DMatrix(X_test), output_margin=True)\n","        \n","        # Calculate the balanced logloss score\n","        ll = scoring(y=y_test, p=preds)\n","        mean_balanced_logloss_score.append(ll)\n","    \n","    return np.mean(mean_balanced_logloss_score)\n","\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=100)\n","\n","optimization_results = pd.DataFrame([study.trials[i].params for i in range(len(study.trials))])\n","optimization_results['score'] = [study.trials[i].value for i in range(len(study.trials))]\n","optimization_results = optimization_results.sort_values(by='score')\n","\n","optimization_results"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:29:09.310148Z","iopub.status.busy":"2023-06-06T05:29:09.309716Z","iopub.status.idle":"2023-06-06T05:29:09.941396Z","shell.execute_reply":"2023-06-06T05:29:09.940500Z","shell.execute_reply.started":"2023-06-06T05:29:09.310112Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.362118190389319\n"]}],"source":["best_params = study.best_params\n","best_params['disable_default_eval_metric'] = True\n","best_params['verbosity']=0\n","\n","scores = []\n","\n","kf = KFold(10)\n","\n","for train_index, test_index in kf.split(X):\n","\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    sampler_model = RandomOverSampler()\n","    X_re, y_re = sampler_model.fit_resample(X_train, y_train)\n","\n","    dtrain = xgb.DMatrix(X_re, y_re)\n","    dval = xgb.DMatrix(X_test, y_test)\n","\n","    model = xgb.train(params=best_params,\n","                      dtrain=dtrain,\n","                      obj=balancedlogloss,\n","                      feval=balancedlogloss_eval, \n","                      verbose_eval=5,\n","                      num_boost_round=50,\n","                    #   early_stopping_rounds=5\n","                      )\n","    score = scoring(y=y_test, p=model.predict(dval))\n","    scores = scores + [score]\n","  \n","print(np.mean(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T05:28:25.525778Z","iopub.status.busy":"2023-06-06T05:28:25.525003Z","iopub.status.idle":"2023-06-06T05:28:26.215562Z","shell.execute_reply":"2023-06-06T05:28:26.214628Z","shell.execute_reply.started":"2023-06-06T05:28:25.525738Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\tdtrain-balanced_logloss:0.62793\tdval-balanced_logloss:0.66931\n","[5]\tdtrain-balanced_logloss:0.28599\tdval-balanced_logloss:0.45725\n","[9]\tdtrain-balanced_logloss:0.18724\tdval-balanced_logloss:0.37280\n","\n","\n","[0]\tdtrain-balanced_logloss:0.62590\tdval-balanced_logloss:0.62661\n","[5]\tdtrain-balanced_logloss:0.29293\tdval-balanced_logloss:0.41340\n","[9]\tdtrain-balanced_logloss:0.20325\tdval-balanced_logloss:0.38609\n","\n","\n","[0]\tdtrain-balanced_logloss:0.63278\tdval-balanced_logloss:0.68131\n","[5]\tdtrain-balanced_logloss:0.30967\tdval-balanced_logloss:0.52930\n","[9]\tdtrain-balanced_logloss:0.20884\tdval-balanced_logloss:0.55227\n","\n","\n","[0]\tdtrain-balanced_logloss:0.62753\tdval-balanced_logloss:0.66600\n","[5]\tdtrain-balanced_logloss:0.28829\tdval-balanced_logloss:0.55568\n","[9]\tdtrain-balanced_logloss:0.18854\tdval-balanced_logloss:0.59923\n","\n","\n","[0]\tdtrain-balanced_logloss:0.63162\tdval-balanced_logloss:0.67883\n","[5]\tdtrain-balanced_logloss:0.30896\tdval-balanced_logloss:0.39287\n","[9]\tdtrain-balanced_logloss:0.19603\tdval-balanced_logloss:0.26420\n","\n","\n","[0]\tdtrain-balanced_logloss:0.62899\tdval-balanced_logloss:0.62646\n","[5]\tdtrain-balanced_logloss:0.31098\tdval-balanced_logloss:0.27866\n","[9]\tdtrain-balanced_logloss:0.20867\tdval-balanced_logloss:0.17737\n","\n","\n","[0]\tdtrain-balanced_logloss:0.63602\tdval-balanced_logloss:0.64841\n","[5]\tdtrain-balanced_logloss:0.31429\tdval-balanced_logloss:0.38659\n","[9]\tdtrain-balanced_logloss:0.20434\tdval-balanced_logloss:0.33822\n","\n","\n","[0]\tdtrain-balanced_logloss:0.62724\tdval-balanced_logloss:0.68516\n","[5]\tdtrain-balanced_logloss:0.29724\tdval-balanced_logloss:0.44751\n","[9]\tdtrain-balanced_logloss:0.20069\tdval-balanced_logloss:0.39260\n","\n","\n","[0]\tdtrain-balanced_logloss:0.63246\tdval-balanced_logloss:0.67889\n","[5]\tdtrain-balanced_logloss:0.30737\tdval-balanced_logloss:0.39414\n","[9]\tdtrain-balanced_logloss:0.19718\tdval-balanced_logloss:0.31815\n","\n","\n","[0]\tdtrain-balanced_logloss:0.63245\tdval-balanced_logloss:0.67359\n","[5]\tdtrain-balanced_logloss:0.31408\tdval-balanced_logloss:0.48999\n","[9]\tdtrain-balanced_logloss:0.20224\tdval-balanced_logloss:0.48274\n","\n","\n"]}],"source":["test = pd.read_csv('test.csv')\n","test['EJ'].replace(['A', 'B'], [1, 0], inplace=True)\n","\n","test_ej = np.array(pd.get_dummies(test['EJ']))\n","\n","x_test_scaled = scaler.transform(test[x_numerical_columns])\n","\n","X_test = np.append(x_test_scaled, test_ej, axis=1)\n","d_test = xgb.DMatrix(X_test)\n","\n","preds = pd.DataFrame(index=range(test.shape[0]))\n","\n","for i in range(0, 10):\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.05, shuffle=True)\n","    sampler_model = RandomOverSampler()\n","    X_re, y_re = sampler_model.fit_resample(X_train, y_train)\n","\n","    dtrain = xgb.DMatrix(X_re, y_re)\n","    dval = xgb.DMatrix(X_test, y_test)\n","\n","    model = xgb.train(params=best_params,\n","                      dtrain=dtrain,\n","                      obj=balancedlogloss,\n","                      evals=[(dtrain, 'dtrain'), (dval, 'dval')],\n","                      feval=balancedlogloss_eval,\n","                      verbose_eval=5,\n","                      early_stopping_rounds=5\n","                      )\n","\n","    p = expit(model.predict(d_test))\n","    p = pd.Series(p)\n","\n","    preds = pd.concat([preds, p], axis=1)\n","    print('\\n')\n","\n","pred_1 = np.mean(preds, axis=1)\n","pred_0 = 1 - pred_1\n","\n","# pred_0\n","\n","submission = pd.DataFrame(index=test.index, columns=sample_submission.columns)\n","submission['Id'] = test['Id']\n","submission['class_0'] = pred_0\n","submission['class_1'] = pred_1\n","\n","submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
